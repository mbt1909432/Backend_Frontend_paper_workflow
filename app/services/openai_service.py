from typing import List, Dict, Any, Optional, AsyncIterator
from openai import AsyncOpenAI
from app.config.settings import settings
from app.utils.logger import logger
import tiktoken


class OpenAIService:
    """OpenAI æœåŠ¡å°è£…"""
    
    def __init__(self):
        # å¦‚æœé…ç½®äº†è‡ªå®šä¹‰ endpointï¼Œåˆ™ä½¿ç”¨å®ƒï¼ˆç”¨äºæ¨¡å‹è½¬å‘å•†ï¼‰
        client_kwargs = {"api_key": settings.openai_api_key}
        if settings.openai_api_base:
            client_kwargs["base_url"] = settings.openai_api_base
            logger.info(f"Using custom OpenAI API endpoint: {settings.openai_api_base}")
        
        self.client = AsyncOpenAI(**client_kwargs)
        self.default_model = settings.openai_model
        self.default_temperature = settings.openai_temperature
        self.default_max_tokens = settings.openai_max_tokens
    
    def _count_tokens(self, text: str, model: Optional[str] = None) -> int:
        """ä½¿ç”¨ tiktoken ç»Ÿè®¡ token æ•°ï¼ˆä¼˜å…ˆæŒ‰æ¨¡å‹ç¼–ç ï¼Œå¤±è´¥åˆ™å›é€€åˆ°é€šç”¨ç¼–ç ï¼‰"""
        try:
            encoding = tiktoken.encoding_for_model(model or self.default_model)
        except Exception:
            encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))
    
    def _format_messages_for_log(self, messages: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯åˆ—è¡¨ç”¨äºæ—¥å¿—è¾“å‡º"""
        formatted = []
        for msg in messages:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªæ–­æ˜¾ç¤ºï¼ˆå¹¶ç”¨ tiktoken ç»Ÿè®¡ token æ•°ï¼‰
            if len(content) > 200:
                total_tokens = self._count_tokens(content)
                content_preview = content[:200] + f"\n... (truncated, total tokens: {total_tokens})"
            else:
                content_preview = content
            formatted.append(f"  {role}: {content_preview}")
        return "\n".join(formatted)
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model: Optional[str] = None
    ) -> tuple[str, Dict[str, Any]]:
        """
        éæµå¼èŠå¤©å®Œæˆ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            model: æ¨¡å‹åç§°
            
        Returns:
            (response_text, usage_info)
        """
        try:
            # æ‰“å° prompt
            logger.info("=" * 80)
            logger.info("LLM Request (Prompt):")
            logger.info(f"Model: {model or self.default_model}")
            logger.info(f"Temperature: {temperature if temperature is not None else self.default_temperature}")
            logger.info(f"Max Tokens: {max_tokens or self.default_max_tokens}")
            logger.info("Messages:")
            logger.info(self._format_messages_for_log(messages))
            logger.info("ğŸ˜€" * 80)
            
            response = await self.client.chat.completions.create(
                model=model or self.default_model,
                messages=messages,
                temperature=temperature if temperature is not None else self.default_temperature,
                max_tokens=max_tokens or self.default_max_tokens
            )
            
            response_text = response.choices[0].message.content
            usage_info = {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
            
            # æ‰“å° output
            logger.info("=" * 80)
            logger.info("LLM Response (Output):")
            # å¦‚æœè¾“å‡ºå¤ªé•¿ï¼Œæˆªæ–­æ˜¾ç¤ºï¼ˆå¹¶ç”¨ tiktoken ç»Ÿè®¡ token æ•°ï¼‰
            if len(response_text) > 2000:
                total_tokens = self._count_tokens(response_text, model)
                output_preview = response_text[:2000] + f"\n... (truncated, total tokens: {total_tokens})"
                logger.info(output_preview)
            else:
                logger.info(response_text)
            logger.info(f"Usage: {usage_info['total_tokens']} tokens (prompt: {usage_info['prompt_tokens']}, completion: {usage_info['completion_tokens']})")
            logger.info("=" * 80)
            
            return response_text, usage_info
            
        except Exception as e:
            logger.error(f"OpenAI API error: {str(e)}")
            raise
    
    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model: Optional[str] = None
    ) -> AsyncIterator:
        """
        æµå¼èŠå¤©å®Œæˆ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            model: æ¨¡å‹åç§°
            
        Returns:
            OpenAI æµå¼å“åº”è¿­ä»£å™¨ï¼ˆåŒ…è£…åï¼Œä¼šæ”¶é›†å®Œæ•´è¾“å‡ºå¹¶æ‰“å°æ—¥å¿—ï¼‰
        """
        try:
            # æ‰“å° prompt
            logger.info("=" * 80)
            logger.info("LLM Request (Prompt) - Streaming:")
            logger.info(f"Model: {model or self.default_model}")
            logger.info(f"Temperature: {temperature if temperature is not None else self.default_temperature}")
            logger.info(f"Max Tokens: {max_tokens or self.default_max_tokens}")
            logger.info("Messages:")
            logger.info(self._format_messages_for_log(messages))
            logger.info("=" * 80)
            
            stream = await self.client.chat.completions.create(
                model=model or self.default_model,
                messages=messages,
                temperature=temperature if temperature is not None else self.default_temperature,
                max_tokens=max_tokens or self.default_max_tokens,
                stream=True
            )
            
            # åŒ…è£…æµå¼å“åº”ï¼Œæ”¶é›†å®Œæ•´è¾“å‡º
            accumulated_text = ""
            usage_info = None
            
            async def wrapped_stream():
                nonlocal accumulated_text, usage_info
                try:
                    async for chunk in stream:
                        # æ£€æŸ¥æ˜¯å¦æœ‰ usage ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunk ä¸­ï¼‰
                        if hasattr(chunk, 'usage') and chunk.usage:
                            usage_info = {
                                "prompt_tokens": chunk.usage.prompt_tokens,
                                "completion_tokens": chunk.usage.completion_tokens,
                                "total_tokens": chunk.usage.total_tokens
                            }
                        
                        # æ”¶é›†å†…å®¹
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if hasattr(delta, 'content') and delta.content:
                                accumulated_text += delta.content
                        
                        yield chunk
                    
                    # æµç»“æŸåæ‰“å°å®Œæ•´è¾“å‡º
                    logger.info("=" * 80)
                    logger.info("LLM Response (Output) - Streaming Complete:")
                    if len(accumulated_text) > 2000:
                        total_tokens = self._count_tokens(accumulated_text, model)
                        output_preview = accumulated_text[:2000] + f"\n... (truncated, total tokens: {total_tokens})"
                        logger.info(output_preview)
                    else:
                        logger.info(accumulated_text)
                    if usage_info:
                        logger.info(f"Usage: {usage_info['total_tokens']} tokens (prompt: {usage_info['prompt_tokens']}, completion: {usage_info['completion_tokens']})")
                    else:
                        logger.info("Usage: N/A (not provided in stream)")
                    logger.info("=" * 80)
                    
                except Exception as e:
                    logger.error(f"Error in streaming: {str(e)}")
                    raise
            
            logger.info("OpenAI streaming started")
            # è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨å¯¹è±¡ï¼ˆå¯ä»¥ç›´æ¥ç”¨äº async forï¼‰
            return wrapped_stream()
            
        except Exception as e:
            logger.error(f"OpenAI streaming error: {str(e)}")
            raise

