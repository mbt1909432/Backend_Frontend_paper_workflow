from __future__ import annotations

"""
Query â†’ Markdown å·¥ä½œæµçš„å‰ä¸¤æ­¥ï¼ˆåˆç‰ˆï¼‰ï¼š
- è°ƒç”¨ QueryRewriteAgent æŠŠåŸå§‹ query é‡å†™ä¸º 4 æ¡å®Œæ•´æ£€ç´¢çŸ­å¥
- å¯¹æ¯ä¸ªæ£€ç´¢çŸ­å¥è°ƒç”¨ arxiv_service.search_and_download
- å†™å‡º raw_pdfs ç›®å½•ä¸ summary/papers_manifest.json

åç»­ PDF â†’ æ–‡æœ¬ ä¸ Markdown ç”Ÿæˆå¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šé€æ­¥æ‰©å±•ã€‚
"""

import json
import random
import asyncio
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

from app.core.agents.query_rewrite_agent import QueryRewriteAgent
from app.core.agents.vision_agent import VisionAgent
from app.core.agents.methodology_extraction_agent import MethodologyExtractionAgent
from app.core.agents.experiment_extraction_agent import ExperimentExtractionAgent
from app.core.agents.innovation_synthesis_agent import InnovationSynthesisAgent
from app.core.agents.writing.methods_writing_agent import MethodsWritingAgent
from app.core.agents.writing.main_results_writing_agent import MainResultsWritingAgent
from app.core.workflows.postprocess_steps import (
    SessionStepInputs,
    run_pdf_ocr_step,
    run_markdown_emit_step,
    run_innovation_synthesis_step,
    run_methodology_extraction_step,
    run_experiment_extraction_step,
    run_methods_writing_step,
    run_main_results_writing_step,
    _load_local_env_file,
)
from app.config.settings import settings
from app.services.arxiv_service import search_and_download, ArxivPaperMetadata
from app.services.embedding_service import EmbeddingService
from app.services.anthropic_service import AnthropicService
from app.services.openai_service import OpenAIService
from app.utils.file_manager import create_session_folder, save_artifact
from app.utils.logger import logger
from app.utils.pdf_converter import pdf_to_pngs


def _mask_secret(secret: Optional[str]) -> str:
    """Mask long secrets before logging."""
    if not secret:
        return "None"
    if len(secret) <= 8:
        return f"{secret[:2]}***"
    return f"{secret[:4]}...{secret[-4:]}"


class QueryToMarkdownWorkflow:
    """
    Query â†’ Markdown å·¥ä½œæµ

    å½“å‰å®ç°çš„ 6 ä¸ªé˜¶æ®µï¼š
    - rewrite: ä½¿ç”¨ QueryRewriteAgent ç”Ÿæˆ 4 æ¡æ£€ç´¢çŸ­å¥ï¼Œè½ç›˜ rewrite.json
    - search:  å¯¹æ¯æ¡æ£€ç´¢çŸ­å¥æ‰§è¡Œ arXiv æœç´¢ä¸ä¸‹è½½ï¼Œç”Ÿæˆ raw_pdfs/ ä¸ papers_manifest.json
    - ingest_pdf: å¯¹ manifest ä¸­ PDF æ‰§è¡Œ PDFâ†’PNGâ†’OCRï¼Œç”Ÿæˆ processed/paper_{idx}/ ç›®å½•ä¸ pdf_processing.json
    - emit_md: æ ¹æ® OCR æ–‡æœ¬ä¸ metadata ç”Ÿæˆ markdown/paper_*.md ä¸ summary/index.mdã€markdown_emit.json
    - extract_methodology_and_experiment: ä»ç”Ÿæˆçš„ Markdown æ–‡ä»¶ä¸­å¹¶è¡Œæå– problem statement & methodology ä»¥åŠ experimentsï¼Œç”Ÿæˆå¯¹åº” markdown ä¸ artifact JSON æ–‡ä»¶
    - innovation_synthesis: åŸºäºæå–çš„ methodology è¿›è¡Œåˆ›æ–°ç‚¹ç»¼åˆ
    """

    def __init__(
        self,
        query_rewrite_agent: QueryRewriteAgent,
        vision_agent: VisionAgent,
        methodology_extraction_agent: Optional[MethodologyExtractionAgent] = None,
        experiment_extraction_agent: Optional[ExperimentExtractionAgent] = None,
        innovation_agent: Optional[InnovationSynthesisAgent] = None,
        methods_writing_agent: Optional[MethodsWritingAgent] = None,
        main_results_agent: Optional[MainResultsWritingAgent] = None,
        embedding_service: Optional[EmbeddingService] = None,
        max_concurrent_pdfs: int = 2,
        max_concurrent_pages: int = 5,
        max_pages_per_pdf: Optional[int] = 50,
    ):
        self.query_rewrite_agent = query_rewrite_agent
        self.vision_agent = vision_agent
        self.methodology_extraction_agent = methodology_extraction_agent
        self.experiment_extraction_agent = experiment_extraction_agent
        self.innovation_agent = innovation_agent
        self.methods_writing_agent = methods_writing_agent
        self.main_results_agent = main_results_agent
        self.embedding_service = embedding_service or EmbeddingService()
        self.max_concurrent_pdfs = max_concurrent_pdfs
        self.max_concurrent_pages = max_concurrent_pages
        self.max_pages_per_pdf = max_pages_per_pdf

    async def execute(
        self,
        original_query: str,
        session_id: Optional[str] = None,
        username: Optional[str] = None,
        target_paper_count: int = 12,
        per_keyword_max_results: int = 10,
        per_keyword_recent_limit: int = 3,
        skip_dblp_check: bool = False,
        innovation_keywords_override: Optional[List[str]] = None,
        innovation_run_count: int = 1,
        max_pages_per_pdf: Optional[int] = None,
        max_paper_age_years: Optional[int] = 2,
    ) -> Dict[str, Any]:
        # æ‰§è¡Œå®Œæ•´æµç¨‹ï¼š
        # 1) QueryRewriteAgent ç”Ÿæˆ 4 æ¡æ£€ç´¢çŸ­å¥
        # 2) å¯¹æ¯æ¡çŸ­å¥è°ƒç”¨ arXiv æœç´¢ä¸ä¸‹è½½ï¼Œç”Ÿæˆ manifest
        # 3) å¯¹ manifest ä¸­ PDF æ‰§è¡Œ PDFâ†’PNGâ†’OCR
        # 4) ç”Ÿæˆ Markdown ä¸ summary/index.md
        # 5) ä» Markdown æ–‡ä»¶ä¸­æå– problem statement ä¸ methodologyï¼ˆå¦‚æœ agent å·²æä¾›ï¼‰

        # 1. åˆ›å»º session ç›®å½•ï¼ˆå¯¹é½ç°æœ‰ file_manager é€»è¾‘ï¼‰
        session_folder = create_session_folder(session_id, username=username)
        session_id = session_folder.name

        logger.info("=" * 80)
        logger.info(f"Starting Queryâ†’Markdown Workflow (rewrite + arxiv) - Session: {session_id}")
        logger.info("=" * 80)

        logger.info(
            "Config: OpenAI key=%s base=%s model=%s",
            _mask_secret(settings.openai_api_key),
            settings.openai_api_base or "https://api.openai.com/v1",
            settings.openai_model,
        )
        logger.info(
            "Config: Anthropic key=%s base=%s model=%s",
            _mask_secret(settings.anthropic_api_key),
            settings.anthropic_api_base or "https://api.anthropic.com",
            settings.anthropic_model,
        )

        if target_paper_count < 3:
            logger.warning(
                "target_paper_count=%d is below minimum requirement (3). Overriding to 3 for downstream innovation stage.",
                target_paper_count,
            )
            target_paper_count = 3

        artifact_dir = session_folder / "artifact"
        generated_dir = session_folder / "generated"
        artifact_dir.mkdir(parents=True, exist_ok=True)
        generated_dir.mkdir(parents=True, exist_ok=True)

        # -------------------------
        # Step 1: Query Rewrite
        # -------------------------
        logger.info("Step 1: Query rewrite with QueryRewriteAgent")
        rewrite_result = await self.query_rewrite_agent.generate_rewrite(
            original_query=original_query
        )

        rewrite_json = rewrite_result.get("json") or {}
        keywords: List[str] = rewrite_json.get("keywords") or []
        concat_query = " ".join(kw.strip() for kw in keywords if kw.strip())
        if concat_query:
            logger.info(
                "Constructed concat_query (%d chars) for embedding ranking.",
                len(concat_query),
            )
        else:
            logger.warning(
                "concat_query is empty (no rewrite keywords). Will fall back to time-based selection."
            )

        # å°† rewrite ç»“æœå­˜ä¸º artifact/rewrite.jsonï¼ˆå¯¹é½æ–‡æ¡£çº¦å®šï¼Œå¤ç”¨ file_manager.save_artifactï¼‰
        rewrite_artifact_path = save_artifact(
            session_folder=session_folder,
            stage_name="rewrite",
            artifact_data={
                "original_query": original_query,
                "keywords": keywords,
                "agent_payload": rewrite_json,
                "usage": rewrite_result.get("usage"),
            },
        )
        logger.info("âœ“ rewrite.json saved at %s", rewrite_artifact_path)

        # -------------------------
        # Step 2: arXiv æœç´¢ä¸ä¸‹è½½
        # -------------------------
        logger.info("Step 2: arXiv search & download for rewritten queries")

        raw_pdfs_root = session_folder / "raw_pdfs"
        raw_pdfs_root.mkdir(parents=True, exist_ok=True)

        all_papers: List[ArxivPaperMetadata] = []

        for i, kw in enumerate(keywords):
            keyword_dir_name = kw.replace(" ", "_")[:80] or "keyword"
            keyword_outdir = raw_pdfs_root / keyword_dir_name
            logger.info("Running arXiv search for keyword: %s (outdir=%s)", kw, keyword_outdir)

            papers = search_and_download(
                keyword=kw,
                outdir=keyword_outdir,
                max_results=per_keyword_max_results,
                recent_limit=per_keyword_recent_limit,
                filter_surveys=True,
                skip_dblp_check=skip_dblp_check,
            )
            if skip_dblp_check:
                logger.info(
                    "arXiv search finished for keyword '%s': %d papers downloaded (DBLP check skipped)",
                    kw,
                    len(papers),
                )
            else:
                logger.info(
                    "arXiv search finished for keyword '%s': %d papers passed DBLP & downloaded",
                    kw,
                    len(papers),
                )
            all_papers.extend(papers)
            
            # åœ¨å…³é”®è¯ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(keywords) - 1:
                delay_seconds = 3  # 3ç§’å»¶è¿Ÿ
                logger.info(f"ç­‰å¾… {delay_seconds} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯...")
                time.sleep(delay_seconds)

        # å»é‡
        unique_by_id: Dict[str, ArxivPaperMetadata] = {}
        for p in all_papers:
            unique_by_id[p.arxiv_id] = p

        deduped_papers = list(unique_by_id.values())
        logger.info("Deduped papers: %d -> %d", len(all_papers), len(deduped_papers))

        def _to_naive(dt: datetime) -> datetime:
            return dt.replace(tzinfo=None) if dt.tzinfo else dt

        # å¯é€‰çš„æ—¶é—´çª—å£è¿‡æ»¤
        cutoff_date: Optional[datetime] = None
        filtered_by_age: List[ArxivPaperMetadata] = deduped_papers
        if max_paper_age_years is not None and max_paper_age_years > 0:
            cutoff_date = datetime.utcnow() - timedelta(days=365 * max_paper_age_years)
            filtered_by_age = [
                p
                for p in deduped_papers
                if p.published and _to_naive(p.published) >= cutoff_date
            ]
        logger.info(
                "Age filter applied: <= %d years (cutoff=%s). %d papers remain.",
                max_paper_age_years,
                cutoff_date.isoformat(),
                len(filtered_by_age),
            )

        # å¦‚æœè¿‡æ»¤åæ•°é‡ä¸è¶³ï¼Œåˆ™è¡¥é½æ—§è®ºæ–‡
        shortlisted: List[ArxivPaperMetadata] = list(filtered_by_age)
        if len(shortlisted) < target_paper_count:
            logger.info(
                "Only %d papers after age filter; padding with older ones to reach %d target.",
                len(shortlisted),
                target_paper_count,
            )
            seen_ids = {p.arxiv_id for p in shortlisted}
            for p in deduped_papers:
                if p.arxiv_id in seen_ids:
                    continue
                shortlisted.append(p)
                seen_ids.add(p.arxiv_id)
                if len(shortlisted) >= target_paper_count:
                    break

        def _published_ts(paper: ArxivPaperMetadata) -> float:
            if not paper.published:
                return 0.0
            try:
                return paper.published.timestamp()
            except Exception:
                return 0.0

        ranking_strategy = "published_date"
        embedding_model_name: Optional[str] = None

        # åµŒå…¥é‡æ’ï¼ˆè‹¥é…ç½®ä¸” concat_query å¯ç”¨ï¼‰
        if (
            concat_query
            and shortlisted
            and self.embedding_service
            and self.embedding_service.is_configured
        ):
            try:
                ranking_strategy = "embedding"
                embedding_model_name = self.embedding_service.model
                paper_payloads = []
                for paper in shortlisted:
                    summary_snippet = (paper.summary or "")[:1500]
                    payload = f"{paper.title}\n\n{summary_snippet}"
                    paper_payloads.append(payload)

                embeddings = self.embedding_service.embed_texts(
                    [concat_query] + paper_payloads
                )
                query_embedding = embeddings[0]
                paper_embeddings = embeddings[1:]
                for paper, embedding in zip(shortlisted, paper_embeddings):
                    paper.relevance_score = self.embedding_service.cosine_similarity(
                        query_embedding,
                        embedding,
                    )
                shortlisted.sort(
                    key=lambda p: (
                        p.relevance_score is not None,
                        p.relevance_score or -1.0,
                        _published_ts(p),
                    ),
                    reverse=True,
                )
                logger.info(
                    "Embedding-based ranking applied to %d papers (model=%s).",
                    len(shortlisted),
                    embedding_model_name,
                )
            except Exception as exc:  # noqa: BLE001
                ranking_strategy = "published_date"
                embedding_model_name = None
                logger.exception("Embedding ranking failed, fallback to date: %s", exc)

        if ranking_strategy == "published_date":
            shortlisted.sort(key=_published_ts, reverse=True)

        top_papers = shortlisted[:target_paper_count]

        status = "ok" if len(top_papers) >= target_paper_count else "insufficient"

        logger.info(
            "arXiv search summary: total_raw=%d, total_deduped=%d, "
            "selected_for_ocr=%d, status=%s, ranking=%s",
            len(all_papers),
            len(deduped_papers),
            len(top_papers),
            status,
            ranking_strategy,
        )

        manifest_items: List[Dict[str, Any]] = [
            p.to_manifest_dict() for p in top_papers
        ]

        papers_manifest = {
            "original_query": original_query,
            "rewrite_keywords": keywords,
            "per_keyword_max_results": per_keyword_max_results,
            "per_keyword_recent_limit": per_keyword_recent_limit,
            "total_found": len(all_papers),
            "total_deduped": len(deduped_papers),
            "total_after_age_filter": len(filtered_by_age),
             "target_paper_count": target_paper_count,
            "concat_query": concat_query,
            "ranking_strategy": ranking_strategy,
            "embedding_model": embedding_model_name,
            "max_paper_age_years": max_paper_age_years,
            "age_filter_cutoff": cutoff_date.isoformat() if cutoff_date else None,
            "papers": manifest_items,
            "status": status,
        }

        # å­˜åˆ° generated/papers_manifest.jsonï¼ˆå¯¹é½æ–‡æ¡£è§„åˆ’ï¼‰
        manifest_path = generated_dir / "papers_manifest.json"
        manifest_path.write_text(
            json.dumps(papers_manifest, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        logger.info("âœ“ papers_manifest.json saved at %s", manifest_path)

        # -------------------------
        # Step 3: PDF â†’ æ–‡æœ¬ï¼ˆOCRï¼‰
        # -------------------------
        papers_manifest, pdf_processing_results, pdf_processing_artifact_path = (
            await run_pdf_ocr_step(
                session_folder=session_folder,
                vision_agent=self.vision_agent,
                max_concurrent_pdfs=self.max_concurrent_pdfs,
                max_concurrent_pages=self.max_concurrent_pages,
                max_pages_per_pdf=(
                    max_pages_per_pdf
                    if max_pages_per_pdf is not None
                    else self.max_pages_per_pdf
                ),
            )
        )

        # -------------------------
        # Step 4: Markdown ç”Ÿæˆ
        # -------------------------
        logger.info("Step 4: Emit Markdown files from OCR text")

        markdown_items, markdown_emit_artifact_path, index_md_path = (
            await run_markdown_emit_step(
                session_folder=session_folder,
                papers_manifest=papers_manifest,
            )
        )

        step_inputs = SessionStepInputs(
            session_folder=session_folder,
            generated_dir=generated_dir,
            artifact_dir=artifact_dir,
            markdown_items=markdown_items,
            keywords=keywords,
        )

        # -------------------------
        # Step 5: Problem Statement & Methodology æå– + Experiment æå–ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
        # -------------------------
        methodology_extraction_artifact_path: Optional[str] = None
        methodology_items: List[Dict[str, Any]] = []
        experiment_extraction_artifact_path: Optional[str] = None
        experiment_items: List[Dict[str, Any]] = []

        if self.methodology_extraction_agent is not None or self.experiment_extraction_agent is not None:
            logger.info(
                "Step 5: Extract problem statements & methodologies + experiments from Markdown files (parallel)"
            )

            has_methodology = self.methodology_extraction_agent is not None
            has_experiment = self.experiment_extraction_agent is not None

            # å‡†å¤‡å¹¶è¡Œä»»åŠ¡åˆ—è¡¨ï¼ˆåªåŠ å…¥çœŸå®éœ€è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼‰
            gather_tasks = []
            if has_methodology:
                gather_tasks.append(
                    run_methodology_extraction_step(
                        step_inputs=step_inputs,
                        methodology_agent=self.methodology_extraction_agent,
                        max_concurrent_tasks=self.max_concurrent_pdfs,
                    )
                )
            if has_experiment:
                gather_tasks.append(
                    run_experiment_extraction_step(
                        step_inputs=step_inputs,
                        experiment_agent=self.experiment_extraction_agent,
                        max_concurrent_tasks=self.max_concurrent_pdfs,
                    )
                )

            # å¹¶è¡Œæ‰§è¡Œ
            results = await asyncio.gather(*gather_tasks, return_exceptions=True)

            # å¤„ç†ç»“æœï¼Œé¡ºåºä¸ä»»åŠ¡æ·»åŠ é¡ºåºä¸€è‡´
            result_idx = 0
            if has_methodology:
                try:
                    result = results[result_idx]
                    result_idx += 1
                    if isinstance(result, Exception):
                        logger.error("Step 5 methodology extraction failed: %s", result)
                    else:
                        (
                            methodology_extraction_artifact_path,
                            methodology_items,
                        ) = result
                        logger.info(
                            "Methodology artifact: %s",
                            methodology_extraction_artifact_path,
                        )
                except Exception as e:  # noqa: BLE001
                    logger.error("Error processing methodology result: %s", e)

            if has_experiment:
                try:
                    result = results[result_idx]
                    if isinstance(result, Exception):
                        logger.error("Step 5 experiment extraction failed: %s", result)
                    else:
                        (
                            experiment_extraction_artifact_path,
                            experiment_items,
                        ) = result
                        logger.info(
                            "Experiment artifact: %s",
                            experiment_extraction_artifact_path,
                        )
                except Exception as e:  # noqa: BLE001
                    logger.error("Error processing experiment result: %s", e)
        else:
            logger.info(
                "Step 5: Skipped (methodology_extraction_agent and experiment_extraction_agent not provided)"
            )

        # -------------------------
        # Step 6: Innovation synthesis agentï¼ˆ3-paper requirementï¼‰
        # -------------------------
        innovation_artifact_paths: List[str] = []
        if self.innovation_agent is not None:
            if not methodology_items:
                logger.warning(
                    "Innovation agent skipped: methodology step produced 0 eligible entries."
                )
            else:
                innovation_artifact_paths = await run_innovation_synthesis_step(
                    step_inputs=step_inputs,
                    methodology_items=methodology_items,
                    innovation_agent=self.innovation_agent,
                    override_keywords=innovation_keywords_override,
                    run_count=innovation_run_count,
                )
        else:
            logger.info("Step 6: Skipped (innovation_agent not provided)")

        # -------------------------
        # Step 7: Methods writing (LaTeX Methods section)
        # -------------------------
        methods_writing_artifacts: List[str] = []
        if self.methods_writing_agent is not None:
            if not innovation_artifact_paths:
                logger.info(
                    "Step 7: Skipped (no innovation_synthesis artifacts produced in Step 6)."
                )
            else:
                methods_writing_artifacts = await run_methods_writing_step(
                    step_inputs=step_inputs,
                    methods_writing_agent=self.methods_writing_agent,
                    temperature=0.7,
                    max_tokens=20000,
                )
        else:
            logger.info("Step 7: Skipped (methods_writing_agent not provided)")

        # -------------------------
        # Step 8: Main Results writing (LaTeX Main Results section)
        # -------------------------
        main_results_writing_artifacts: List[str] = []
        if self.main_results_agent is not None:
            if not experiment_items:
                logger.info(
                    "Step 8: Skipped (no experiment_items produced in Step 5 experiment extraction)."
                )
            elif not innovation_artifact_paths:
                logger.info(
                    "Step 8: Skipped (no innovation_synthesis artifacts to pair with experiments)."
                )
            else:
                main_results_writing_artifacts = (
                    await run_main_results_writing_step(
                        step_inputs=step_inputs,
                        main_results_agent=self.main_results_agent,
                        temperature=0.6,
                        max_tokens=40000,
                        model=None,
                    )
                )
        else:
            logger.info("Step 8: Skipped (main_results_agent not provided)")

        return {
            "session_id": session_id,
            "session_folder": str(session_folder),
            "rewrite_artifact": str(rewrite_artifact_path),
            "papers_manifest": str(manifest_path),
            "pdf_processing_artifact": str(pdf_processing_artifact_path),
            "markdown_emit_artifact": str(markdown_emit_artifact_path),
            "index_md": str(index_md_path),
            "methodology_extraction_artifact": methodology_extraction_artifact_path,
            "experiment_extraction_artifact": experiment_extraction_artifact_path,
            "methodology_items": methodology_items,
            "experiment_items": experiment_items,
            "innovation_artifacts": innovation_artifact_paths,
            "innovation_artifact": innovation_artifact_paths[0] if innovation_artifact_paths else None,
            "methods_writing_artifacts": methods_writing_artifacts,
            "main_results_writing_artifacts": main_results_writing_artifacts,
            "status": status,
        }


_VISION_TEST_IMAGE_PATH = Path(__file__).resolve().parents[3] / "lab" /"img.png" #"arxiv_2506.06962v3_page_18.png"


async def _test_anthropic_connectivity(anthropic_service: AnthropicService) -> bool:
    """
    å‘é€ä¸€ä¸ªæå°çš„ messages.create è¯·æ±‚ï¼Œå¿«é€ŸéªŒè¯ Anthropic API æ˜¯å¦å¯ç”¨ã€‚
    é‡åˆ°æ— æ•ˆ token/ç½‘ç»œé”™è¯¯æ—¶æå‰ç»ˆæ­¢ main æµ‹è¯•ï¼Œé¿å…æ•´æ¡æµæ°´çº¿å¤±è´¥åˆ° OCR é˜¶æ®µæ‰å‘ç°ã€‚
    """
    logger.info("Running Anthropic connectivity test...")
    try:
        await anthropic_service.messages_create(
            messages=[{"role": "user", "content": "Ping. Reply with PONG."}],
            temperature=0,
            max_tokens=5,
            model=settings.anthropic_model,
            system="You are a simple health-check bot. Respond with 'PONG'.",
        )
        logger.info("Anthropic connectivity test succeeded.")
        return True
    except Exception as exc:
        logger.error("Anthropic connectivity test failed: %s", exc)
        return False


async def _test_vision_agent(vision_agent: VisionAgent) -> bool:
    """
    ä½¿ç”¨ lab/1.png è°ƒç”¨ VisionAgent.analyze_imageï¼ŒéªŒè¯ Anthropic è¯»å›¾æ¥å£è¿é€šæ€§ã€‚
    å›¾ç‰‡å†…å®¹ä¸ºé¡¹ç›®å†…ç½®ç¤ºä¾‹ï¼Œä»…ç”¨äºç¡®è®¤ API å¯æ¥å—å›¾ç‰‡è¾“å…¥å¹¶è¿”å›æ–‡æœ¬ã€‚
    """
    logger.info("Running Anthropic vision (image/OCR) test...")
    if not _VISION_TEST_IMAGE_PATH.exists():
        logger.error("Vision test image not found: %s", _VISION_TEST_IMAGE_PATH)
        return False
    try:
        test_image_bytes = _VISION_TEST_IMAGE_PATH.read_bytes()
        result = await vision_agent.analyze_image(
            text_prompt="å¦‚æœä½ çœ‹å¾—åˆ°å›¾ç‰‡ å›å¤pong",
            images=[test_image_bytes],
            temperature=0,
            max_tokens=10,
        )
        logger.info("Vision test succeeded. ResponseğŸ˜€: %s", result)
        return True
    except Exception as exc:
        logger.error("Anthropic vision test failed: %s", exc)
        return False


async def main() -> None:
    """
    ç®€å•æœ¬åœ°æµ‹è¯•å…¥å£ï¼šç›´æ¥åœ¨å½“å‰è„šæœ¬å†…è·‘ä¸€æ¬¡ Query â†’ Markdown å·¥ä½œæµã€‚
    ä¸ä½¿ç”¨ CLI/argparseï¼Œæ–¹ä¾¿åœ¨ IDE ä¸­è¿è¡Œå’Œæ–­ç‚¹è°ƒè¯•ã€‚
    
    å‚æ•°è®¾ç½®è¯´æ˜ï¼š
    ============
    1. session_id å’Œè¾“å‡ºä½ç½®çš„å…³ç³»ï¼š
       - å¦‚æœ test_session_id = Noneï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆæ ¼å¼ä¸º session_{timestamp}_{uuid} çš„ session_id
       - ä¾‹å¦‚ï¼šsession_20251127_112630_748edba5ï¼ˆ2025å¹´11æœˆ27æ—¥ 11:26:30ï¼ŒUUIDå‰8ä½ï¼‰
       - session_id å†³å®šäº†æ‰€æœ‰è¾“å‡ºæ–‡ä»¶çš„å­˜å‚¨ä½ç½®ï¼š{output_dir}/{username}/{session_id}/
       - æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆPDFã€BibTeXã€Markdownç­‰ï¼‰éƒ½ä¼šä¿å­˜åœ¨è¿™ä¸ª session æ–‡ä»¶å¤¹ä¸‹
    
    2. skip_dblp_check å‚æ•°çš„å½±å“ï¼š
       - skip_dblp_check=Falseï¼ˆé»˜è®¤ï¼‰ï¼šåªä¸‹è½½åœ¨ DBLP ä¸­æœ‰åŒ¹é…çš„è®ºæ–‡ï¼Œä½¿ç”¨ DBLP çš„ BibTeX
       - skip_dblp_check=Trueï¼šè·³è¿‡ DBLP æ£€æŸ¥ï¼Œä¸‹è½½æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œä½¿ç”¨ arXiv ç”Ÿæˆçš„ BibTeX
       - è®¾ç½®ä¸º True æ—¶ï¼Œå¯èƒ½ä¼šä¸‹è½½æ›´å¤šè®ºæ–‡ï¼ˆå› ä¸ºä¸é™åˆ¶ DBLP åŒ¹é…ï¼‰ï¼Œä½† BibTeX è´¨é‡å¯èƒ½è¾ƒä½
    
    3. è®ºæ–‡æ•°é‡æ§åˆ¶å‚æ•°ï¼š
       - target_paper_count=4ï¼šæœ€ç»ˆä¿ç•™çš„è®ºæ–‡æ•°é‡ï¼ˆå»é‡åï¼ŒæŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼Œå–å‰ N ç¯‡ï¼‰
       - per_keyword_max_results=3ï¼šæ¯ä¸ªå…³é”®è¯æœç´¢æ—¶è¿”å›çš„æœ€å¤§ç»“æœæ•°
       - per_keyword_recent_limit=3ï¼šæ¯ä¸ªå…³é”®è¯åªè€ƒè™‘æœ€è¿‘ N ç¯‡è®ºæ–‡
       - å®é™…æµç¨‹ï¼š4ä¸ªå…³é”®è¯ Ã— æ¯ä¸ªæœ€å¤š3ç¯‡ = æœ€å¤š12ç¯‡ â†’ å»é‡ â†’ æŒ‰æ—¶é—´æ’åº â†’ å–å‰4ç¯‡
    
    4. è¾“å‡ºæ–‡ä»¶ä½ç½®ï¼ˆä»¥ session_20251127_112630_748edba5 ä¸ºä¾‹ï¼‰ï¼š
       - raw_pdfs/ï¼šä¸‹è½½çš„åŸå§‹ PDF æ–‡ä»¶
       - generated/papers_manifest.jsonï¼šè®ºæ–‡æ¸…å•ï¼ˆåŒ…å«å…ƒæ•°æ®ã€è·¯å¾„ç­‰ï¼‰
       - generated/markdown/ï¼šç”Ÿæˆçš„ Markdown æ–‡ä»¶
       - generated/index.mdï¼šæ±‡æ€»ç´¢å¼•æ–‡ä»¶
       - artifact/ï¼šä¸­é—´äº§ç‰©ï¼ˆrewrite.jsonã€pdf_processing.json ç­‰ï¼‰
    """
    start_time = time.perf_counter()
    try:
        #test_query="""sciäºŒåŒº[é¢†åŸŸéœ€æ±‚]Autonomous driving safety"""

        # test_query="""sciäºŒåŒº[é¢†åŸŸéœ€æ±‚]AI safety protection algorithms"""

        # test_query="""Cloud computing + autonomous vehicles"""
#         test_query="""Adaptive Multi-Agent Embodied AI with Cross-Domain Visual Planning
#
#
# Current embodied AI systems cannot integrate visual understanding, cross-domain planning, and multi-agent coordination, limiting their ability to perform complex real-world tasks that require both physical actions and digital information retrieval.
#
#
# We solve the problem of fragmented embodied AI systems that cannot handle complex real-world tasks requiring visual understanding, web information, and coordinated actions. Current methods fail because they use fixed visual processing that misses temporal dynamics, make poor decisions about when to switch between physical and digital actions, and lack fault tolerance in multi-agent scenarios. Our method works in three stages: (1) Adaptive visual processing that samples video frames at 1-10fps based on motion detection and uses curriculum learning to adjust training difficulty, (2) Confidence-based cross-domain planning that calculates uncertainty scores to decide when to switch between physical actions and web queries, and (3) Fault-tolerant multi-agent coordination with heartbeat monitoring that detects agent failures in 5 seconds and reassigns tasks automatically. To implement this, we extend VideoLLaMA3 with adaptive sampling, add entropy-based confidence estimation to cross-domain planners, and build heartbeat monitoring into ROS 2 agent frameworks. We test on cooking tasks (using recipes from web), navigation with real-time map data, and warehouse coordination scenarios using AI2-THOR simulation and real robot platforms. The system needs PyTorch, ROS 2, web API access, 12GB GPU memory, and takes 2-3 days to train on video datasets with multi-agent interaction logs. Success is measured by task completion rate, domain switching accuracy, and system uptime during agent failures."""


        # test_query="""logistics: A Multi-Agent Predictive QptimizationFramework"""
        # test_query = """Autonomous driving safety"""
        # test_query = """AI safety protection algorithms"""
        test_query = """Data Science Applications in Social Networks and Advertising: A Causal Inference Approach"""
        test_username = "2025_12_4"
        test_session_id: Optional[str] = None  # None æ—¶ä¼šè‡ªåŠ¨ç”Ÿæˆï¼Œæ ¼å¼ï¼šsession_{timestamp}_{uuid}

        _load_local_env_file()

        # æ„é€ ä¾èµ–
        openai_service = OpenAIService()
        query_agent = QueryRewriteAgent(openai_service=openai_service)
        methodology_agent = MethodologyExtractionAgent(openai_service=openai_service)
        experiment_agent = ExperimentExtractionAgent(openai_service=openai_service)
        innovation_agent = InnovationSynthesisAgent(openai_service=openai_service)
        methods_writing_agent = MethodsWritingAgent(openai_service=openai_service)
        main_results_agent = MainResultsWritingAgent(openai_service=openai_service)

        anthropic_service = AnthropicService()
        vision_agent = VisionAgent(anthropic_service=anthropic_service)


        if not await _test_anthropic_connectivity(anthropic_service):
            logger.error("Abort workflow run due to Anthropic connectivity failure.")
            return
        if not await _test_vision_agent(vision_agent):
            logger.error("Abort workflow run due to Anthropic vision failure.")
            return

        workflow = QueryToMarkdownWorkflow(
            query_rewrite_agent=query_agent,
            vision_agent=vision_agent,
            methodology_extraction_agent=methodology_agent,
            # experiment_extraction_agent=experiment_agent,#TODOï¼šæå–å¾ˆæ…¢ æå–é—®é¢˜ è¿˜æœ‰referenceé˜²æ­¢æå–
            innovation_agent=innovation_agent,
            # methods_writing_agent=methods_writing_agent,
            # main_results_agent=main_results_agent,
            max_concurrent_pdfs=2,
            max_concurrent_pages=2,  # æ¯ç¯‡è®ºæ–‡åŒæ—¶å¤„ç†çš„é¡µé¢æ•°
        )

        result = await workflow.execute(
            original_query=test_query,
            session_id=test_session_id,  # None æ—¶è‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚ï¼šsession_20251127_112630_748edba5
            username=test_username,
            target_paper_count=3,  # æœ€åéœ€è¦çš„æ•°é‡ï¼ˆå»é‡åæŒ‰æ—¶é—´æ’åºå–å‰ N ç¯‡ï¼‰
            per_keyword_max_results=10,  # æ¯ä¸ªå…³é”®è¯æœ€å¤§çš„æœç´¢ç»“æœ
            per_keyword_recent_limit=10,  # æ¯ä¸ªå…³é”®è¯åªè€ƒè™‘æœ€è¿‘ N ç¯‡
            skip_dblp_check=True,  # è®¾ç½®ä¸º True å¯è·³è¿‡ DBLP æ£€æŸ¥ï¼ˆä¼šä¸‹è½½æ›´å¤šè®ºæ–‡ï¼Œä½†ä½¿ç”¨ arXiv BibTeXï¼‰
            max_paper_age_years=3
        )

        logger.info("Queryâ†’Markdown workflow finished.")
        logger.info("Session folder: %s", result["session_folder"])
        logger.info("rewrite_artifact: %s", result["rewrite_artifact"])
        logger.info("papers_manifest: %s", result["papers_manifest"])
        logger.info("pdf_processing_artifact: %s", result["pdf_processing_artifact"])
        logger.info("markdown_emit_artifact: %s", result["markdown_emit_artifact"])
        logger.info("index_md: %s", result["index_md"])
        if result.get("methodology_extraction_artifact"):
            logger.info("methodology_extraction_artifact: %s", result["methodology_extraction_artifact"])
        if result.get("experiment_extraction_artifact"):
            logger.info("experiment_extraction_artifact: %s", result["experiment_extraction_artifact"])
        innovation_artifacts = result.get("innovation_artifacts") or []
        if innovation_artifacts:
            logger.info("innovation_artifacts: %s", innovation_artifacts)
    finally:
        elapsed = time.perf_counter() - start_time
        logger.info(
            "Queryâ†’Markdown workflow total runtime: %.2f seconds (â‰ˆ%.2f minutes)",
            elapsed,
            elapsed / 60,
        )


if __name__ == "__main__":
    asyncio.run(main())


