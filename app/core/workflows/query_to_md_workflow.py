from __future__ import annotations

"""
Query â†’ Markdown å·¥ä½œæµçš„å‰ä¸¤æ­¥ï¼ˆåˆç‰ˆï¼‰ï¼š
- è°ƒç”¨ QueryRewriteAgent æŠŠåŸå§‹ query é‡å†™ä¸º 4 æ¡å®Œæ•´æ£€ç´¢çŸ­å¥
- å¯¹æ¯ä¸ªæ£€ç´¢çŸ­å¥è°ƒç”¨ arxiv_service.search_and_download
- å†™å‡º raw_pdfs ç›®å½•ä¸ summary/papers_manifest.json

åç»­ PDF â†’ æ–‡æœ¬ ä¸ Markdown ç”Ÿæˆå¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šé€æ­¥æ‰©å±•ã€‚
"""

import json
import random
import asyncio
import time
from datetime import datetime
from pathlib import Path
import re
from typing import Any, Dict, List, Optional

from app.core.agents.query_rewrite_agent import QueryRewriteAgent
from app.core.agents.vision_agent import VisionAgent
from app.core.agents.methodology_extraction_agent import MethodologyExtractionAgent
from app.core.agents.experiment_extraction_agent import ExperimentExtractionAgent
from app.core.agents.innovation_synthesis_agent import InnovationSynthesisAgent
from app.core.workflows.postprocess_steps import (
    SessionStepInputs,
    run_innovation_synthesis_step,
    run_methodology_extraction_step,
    run_experiment_extraction_step,
    _load_local_env_file,
)
from app.config.settings import settings
from app.services.arxiv_service import search_and_download, ArxivPaperMetadata
from app.services.anthropic_service import AnthropicService
from app.services.openai_service import OpenAIService
from app.utils.file_manager import create_session_folder, save_artifact
from app.utils.logger import logger
from app.utils.pdf_converter import pdf_to_pngs


def _mask_secret(secret: Optional[str]) -> str:
    """Mask long secrets before logging."""
    if not secret:
        return "None"
    if len(secret) <= 8:
        return f"{secret[:2]}***"
    return f"{secret[:4]}...{secret[-4:]}"


class QueryToMarkdownWorkflow:
    """
    Query â†’ Markdown å·¥ä½œæµ

    å½“å‰å®ç°çš„ 6 ä¸ªé˜¶æ®µï¼š
    - rewrite: ä½¿ç”¨ QueryRewriteAgent ç”Ÿæˆ 4 æ¡æ£€ç´¢çŸ­å¥ï¼Œè½ç›˜ rewrite.json
    - search:  å¯¹æ¯æ¡æ£€ç´¢çŸ­å¥æ‰§è¡Œ arXiv æœç´¢ä¸ä¸‹è½½ï¼Œç”Ÿæˆ raw_pdfs/ ä¸ papers_manifest.json
    - ingest_pdf: å¯¹ manifest ä¸­ PDF æ‰§è¡Œ PDFâ†’PNGâ†’OCRï¼Œç”Ÿæˆ processed/paper_{idx}/ ç›®å½•ä¸ pdf_processing.json
    - emit_md: æ ¹æ® OCR æ–‡æœ¬ä¸ metadata ç”Ÿæˆ markdown/paper_*.md ä¸ summary/index.mdã€markdown_emit.json
    - extract_methodology_and_experiment: ä»ç”Ÿæˆçš„ Markdown æ–‡ä»¶ä¸­å¹¶è¡Œæå– problem statement & methodology ä»¥åŠ experimentsï¼Œç”Ÿæˆå¯¹åº” markdown ä¸ artifact JSON æ–‡ä»¶
    - innovation_synthesis: åŸºäºæå–çš„ methodology è¿›è¡Œåˆ›æ–°ç‚¹ç»¼åˆ
    """

    def __init__(
        self,
        query_rewrite_agent: QueryRewriteAgent,
        vision_agent: VisionAgent,
        methodology_extraction_agent: Optional[MethodologyExtractionAgent] = None,
        experiment_extraction_agent: Optional[ExperimentExtractionAgent] = None,
        innovation_agent: Optional[InnovationSynthesisAgent] = None,
        max_concurrent_pdfs: int = 2,
        max_concurrent_pages: int = 5,
        max_pages_per_pdf: Optional[int] = 50,
    ):
        self.query_rewrite_agent = query_rewrite_agent
        self.vision_agent = vision_agent
        self.methodology_extraction_agent = methodology_extraction_agent
        self.experiment_extraction_agent = experiment_extraction_agent
        self.innovation_agent = innovation_agent
        self.max_concurrent_pdfs = max_concurrent_pdfs
        self.max_concurrent_pages = max_concurrent_pages
        self.max_pages_per_pdf = max_pages_per_pdf

    async def execute(
        self,
        original_query: str,
        session_id: Optional[str] = None,
        username: Optional[str] = None,
        target_paper_count: int = 12,
        per_keyword_max_results: int = 10,
        per_keyword_recent_limit: int = 3,
        skip_dblp_check: bool = False,
        innovation_keywords_override: Optional[List[str]] = None,
        innovation_run_count: int = 1,
        max_pages_per_pdf: Optional[int] = None,
    ) -> Dict[str, Any]:
        # æ‰§è¡Œå®Œæ•´æµç¨‹ï¼š
        # 1) QueryRewriteAgent ç”Ÿæˆ 4 æ¡æ£€ç´¢çŸ­å¥
        # 2) å¯¹æ¯æ¡çŸ­å¥è°ƒç”¨ arXiv æœç´¢ä¸ä¸‹è½½ï¼Œç”Ÿæˆ manifest
        # 3) å¯¹ manifest ä¸­ PDF æ‰§è¡Œ PDFâ†’PNGâ†’OCR
        # 4) ç”Ÿæˆ Markdown ä¸ summary/index.md
        # 5) ä» Markdown æ–‡ä»¶ä¸­æå– problem statement ä¸ methodologyï¼ˆå¦‚æœ agent å·²æä¾›ï¼‰

        # 1. åˆ›å»º session ç›®å½•ï¼ˆå¯¹é½ç°æœ‰ file_manager é€»è¾‘ï¼‰
        session_folder = create_session_folder(session_id, username=username)
        session_id = session_folder.name

        logger.info("=" * 80)
        logger.info(f"Starting Queryâ†’Markdown Workflow (rewrite + arxiv) - Session: {session_id}")
        logger.info("=" * 80)

        logger.info(
            "Config: OpenAI key=%s base=%s model=%s",
            _mask_secret(settings.openai_api_key),
            settings.openai_api_base or "https://api.openai.com/v1",
            settings.openai_model,
        )
        logger.info(
            "Config: Anthropic key=%s base=%s model=%s",
            _mask_secret(settings.anthropic_api_key),
            settings.anthropic_api_base or "https://api.anthropic.com",
            settings.anthropic_model,
        )

        if target_paper_count < 3:
            logger.warning(
                "target_paper_count=%d is below minimum requirement (3). Overriding to 3 for downstream innovation stage.",
                target_paper_count,
            )
            target_paper_count = 3

        artifact_dir = session_folder / "artifact"
        generated_dir = session_folder / "generated"
        artifact_dir.mkdir(parents=True, exist_ok=True)
        generated_dir.mkdir(parents=True, exist_ok=True)

        # -------------------------
        # Step 1: Query Rewrite
        # -------------------------
        logger.info("Step 1: Query rewrite with QueryRewriteAgent")
        rewrite_result = await self.query_rewrite_agent.generate_rewrite(
            original_query=original_query
        )

        rewrite_json = rewrite_result.get("json") or {}
        keywords: List[str] = rewrite_json.get("keywords") or []

        # å°† rewrite ç»“æœå­˜ä¸º artifact/rewrite.jsonï¼ˆå¯¹é½æ–‡æ¡£çº¦å®šï¼Œå¤ç”¨ file_manager.save_artifactï¼‰
        rewrite_artifact_path = save_artifact(
            session_folder=session_folder,
            stage_name="rewrite",
            artifact_data={
                "original_query": original_query,
                "keywords": keywords,
                "agent_payload": rewrite_json,
                "usage": rewrite_result.get("usage"),
            },
        )
        logger.info("âœ“ rewrite.json saved at %s", rewrite_artifact_path)

        # -------------------------
        # Step 2: arXiv æœç´¢ä¸ä¸‹è½½
        # -------------------------
        logger.info("Step 2: arXiv search & download for rewritten queries")

        raw_pdfs_root = session_folder / "raw_pdfs"
        raw_pdfs_root.mkdir(parents=True, exist_ok=True)

        all_papers: List[ArxivPaperMetadata] = []

        for i, kw in enumerate(keywords):
            keyword_dir_name = kw.replace(" ", "_")[:80] or "keyword"
            keyword_outdir = raw_pdfs_root / keyword_dir_name
            logger.info("Running arXiv search for keyword: %s (outdir=%s)", kw, keyword_outdir)

            papers = search_and_download(
                keyword=kw,
                outdir=keyword_outdir,
                max_results=per_keyword_max_results,
                recent_limit=per_keyword_recent_limit,
                filter_surveys=True,
                skip_dblp_check=skip_dblp_check,
            )
            if skip_dblp_check:
                logger.info(
                    "arXiv search finished for keyword '%s': %d papers downloaded (DBLP check skipped)",
                    kw,
                    len(papers),
                )
            else:
                logger.info(
                    "arXiv search finished for keyword '%s': %d papers passed DBLP & downloaded",
                    kw,
                    len(papers),
                )
            all_papers.extend(papers)
            
            # åœ¨å…³é”®è¯ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(keywords) - 1:
                delay_seconds = 3  # 3ç§’å»¶è¿Ÿ
                logger.info(f"ç­‰å¾… {delay_seconds} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯...")
                time.sleep(delay_seconds)

        # å»é‡å¹¶æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼Œä¿ç•™æœ€è¿‘ target_paper_count ç¯‡
        unique_by_id: Dict[str, ArxivPaperMetadata] = {}
        for p in all_papers:
            unique_by_id[p.arxiv_id] = p

        deduped_papers = list(unique_by_id.values())
        deduped_papers.sort(key=lambda p: p.published or 0, reverse=True)
        top_papers = deduped_papers[:target_paper_count]

        status = "ok"
        if len(top_papers) < target_paper_count:
            status = "insufficient"

        logger.info(
            "arXiv search summary: total_raw=%d, total_deduped=%d, selected_for_ocr=%d, status=%s",
            len(all_papers),
            len(deduped_papers),
            len(top_papers),
            status,
        )

        manifest_items: List[Dict[str, Any]] = [
            p.to_manifest_dict() for p in top_papers
        ]

        papers_manifest = {
            "original_query": original_query,
            "rewrite_keywords": keywords,
            "per_keyword_max_results": per_keyword_max_results,
            "per_keyword_recent_limit": per_keyword_recent_limit,
            "total_found": len(all_papers),
            "total_deduped": len(deduped_papers),
             "target_paper_count": target_paper_count,
            "papers": manifest_items,
            "status": status,
        }

        # å­˜åˆ° generated/papers_manifest.jsonï¼ˆå¯¹é½æ–‡æ¡£è§„åˆ’ï¼‰
        manifest_path = generated_dir / "papers_manifest.json"
        manifest_path.write_text(
            json.dumps(papers_manifest, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        logger.info("âœ“ papers_manifest.json saved at %s", manifest_path)

        # -------------------------
        # Step 3: PDF â†’ æ–‡æœ¬ï¼ˆOCRï¼‰
        # -------------------------
        logger.info("Step 3: PDF â†’ Text via pdf_to_pngs + VisionAgent")

        processed_root = session_folder / "processed"
        processed_root.mkdir(parents=True, exist_ok=True)

        resolved_page_limit: Optional[int] = (
            max_pages_per_pdf if max_pages_per_pdf is not None else self.max_pages_per_pdf
        )
        if resolved_page_limit is not None and resolved_page_limit <= 0:
            resolved_page_limit = None

        async def process_single_paper(idx: int, paper: Dict[str, Any]) -> Dict[str, Any]:
            paper_id = paper.get("arxiv_id") or f"paper_{idx:02d}"
            paper_status = paper.get("status", "ok")
            result: Dict[str, Any] = {
                "index": idx,
                "arxiv_id": paper_id,
                "pdf_path": paper.get("pdf_path"),
                "status": paper_status,
                "page_count": 0,
                "usage": {
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "total_tokens": 0,
                },
                "error": None,
            }

            if paper_status != "ok":
                logger.warning(
                    "Skip PDF processing for paper %s (status=%s)",
                    paper_id,
                    paper_status,
                )
                return result

            pdf_path = paper.get("pdf_path")
            if not pdf_path or not Path(pdf_path).exists():
                logger.error("PDF not found for paper %s: %s", paper_id, pdf_path)
                paper["status"] = "failed"
                result["status"] = "failed"
                result["error"] = f"PDF not found: {pdf_path}"
                return result

            paper_dir = processed_root / f"paper_{idx:02d}"
            images_dir = paper_dir / "images"
            ocr_dir = paper_dir / "ocr"
            logs_dir = paper_dir / "logs"
            images_dir.mkdir(parents=True, exist_ok=True)
            ocr_dir.mkdir(parents=True, exist_ok=True)
            logs_dir.mkdir(parents=True, exist_ok=True)

            logger.info(
                "Start PDFâ†’PNGâ†’OCR for paper #%d (%s) into %s",
                idx,
                paper_id,
                paper_dir,
            )

            try:
                png_paths = pdf_to_pngs(
                    pdf_path=str(pdf_path),
                    output_dir=str(images_dir),
                    dpi=300,
                )
            except Exception as e:
                logger.exception("pdf_to_pngs failed for %s: %s", paper_id, e)
                paper["status"] = "failed"
                result["status"] = "failed"
                result["error"] = f"pdf_to_pngs failed: {e}"
                return result

            if not png_paths:
                logger.error("No PNG pages generated for %s", paper_id)
                paper["status"] = "failed"
                result["status"] = "failed"
                result["error"] = "No PNG pages generated"
                return result

            logger.info("PDF %s converted to %d PNG pages", paper_id, len(png_paths))

            def _page_sort_key(path: str) -> tuple[int, str]:
                """
                Extract numeric page index if present to keep OCR ordering consistent.
                Falls back to lexical order to avoid crashes on unexpected filenames.
                """
                stem = Path(path).stem
                match = re.search(r"_page_(\d+)", stem)
                if match:
                    return (int(match.group(1)), stem)
                return (0, stem)

            sorted_png_paths = sorted(png_paths, key=_page_sort_key)
            if resolved_page_limit is not None:
                limited_paths = sorted_png_paths[:resolved_page_limit]
                if len(limited_paths) < len(sorted_png_paths):
                    logger.info(
                        "Limiting OCR for %s to first %d pages (total=%d)",
                        paper_id,
                        len(limited_paths),
                        len(sorted_png_paths),
                    )
                sorted_png_paths = limited_paths

            result["page_count"] = len(sorted_png_paths)
            if not sorted_png_paths:
                logger.error("No PNG pages selected for OCR for %s", paper_id)
                paper["status"] = "failed"
                result["status"] = "failed"
                result["error"] = "No PNG pages selected for OCR"
                return result

            # é¡µé¢çº§å¹¶å‘å¤„ç†ï¼šä½¿ç”¨ asyncio.gather å¹¶å‘å¤„ç†æ‰€æœ‰é¡µé¢
            page_semaphore = asyncio.Semaphore(self.max_concurrent_pages)

            async def process_single_page(page_idx: int, png_path: str) -> tuple[int, str, dict, Optional[str]]:
                """
                å¤„ç†å•é¡µ OCR
                è¿”å›: (page_idx, text, usage, error)
                """
                async with page_semaphore:
                    try:
                        logger.info(
                            "OCR on paper %s page %d/%d: %s",
                            paper_id,
                            page_idx,
                            len(sorted_png_paths),
                            png_path,
                        )
                        ocr_prompt = (
                            "è¯·ç›´æ¥è¾“å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ã€å›¾è¡¨ã€è¡¨æ ¼ã€å…¬å¼ç­‰ï¼Œ"
                            "ä¸è¦æ·»åŠ ä»»ä½•æè¿°ã€è¯´æ˜æˆ–è§£é‡Šã€‚ä¿æŒåŸæœ‰çš„ç»“æ„å’Œæ ¼å¼ä¿¡æ¯ã€‚"
                        )
                        ocr_result = await self.vision_agent.extract_text_from_image(
                            image=png_path,
                            text_prompt=ocr_prompt,
                            temperature=0.3,
                            max_tokens=10000,
                            model=None,
                        )
                        text = ocr_result.get("response") or ""
                        usage = ocr_result.get("usage") or {}
                        
                        logger.info(
                            "OCR completed for paper %s page %d/%d",
                            paper_id,
                            page_idx,
                            len(sorted_png_paths),
                        )
                        
                        return (page_idx, text, usage, None)
                    except Exception as e:
                        logger.exception(
                            "OCR failed on paper %s page %d: %s", paper_id, page_idx, e
                        )
                        return (page_idx, "", {}, f"OCR failed on page {page_idx}: {e}")

            # å¹¶å‘å¤„ç†æ‰€æœ‰é¡µé¢
            logger.info(
                "Starting concurrent OCR for paper %s: %d pages (max_concurrent=%d)",
                paper_id,
                len(sorted_png_paths),
                self.max_concurrent_pages,
            )
            
            page_results = await asyncio.gather(
                *[
                    process_single_page(page_idx, png_path)
                    for page_idx, png_path in enumerate(sorted_png_paths, start=1)
                ],
                return_exceptions=True,
            )

            # æŒ‰é¡µé¢ç´¢å¼•æ’åºå¹¶å¤„ç†ç»“æœï¼ˆç¡®ä¿é¡µé¢é¡ºåºæ­£ç¡®ï¼‰
            page_texts: List[str] = [""] * len(sorted_png_paths)
            failed_pages: List[tuple[int, str]] = []
            
            # è¿‡æ»¤å¼‚å¸¸å¹¶æ’åºï¼ˆæŒ‰ page_idxï¼‰
            valid_results = []
            for page_result in page_results:
                if isinstance(page_result, Exception):
                    logger.error(
                        "Unexpected exception in page processing: %s", page_result
                    )
                    continue
                valid_results.append(page_result)
            
            # æŒ‰ page_idx æ’åºï¼Œç¡®ä¿é¡µé¢é¡ºåºæ­£ç¡®
            valid_results.sort(key=lambda x: x[0])

            for page_idx, text, usage, error in valid_results:
                if error:
                    failed_pages.append((page_idx, error))
                    result["status"] = "failed"
                    if not result.get("error"):
                        result["error"] = error
                else:
                    # ç´¯è®¡ usage
                    result["usage"]["input_tokens"] += usage.get("input_tokens", 0)
                    result["usage"]["output_tokens"] += usage.get("output_tokens", 0)
                    result["usage"]["total_tokens"] += usage.get("total_tokens", 0)
                    
                    # ä¿å­˜åˆ°å¯¹åº”ä½ç½®ï¼ˆpage_idx ä» 1 å¼€å§‹ï¼Œæ•°ç»„ä» 0 å¼€å§‹ï¼‰
                    page_texts[page_idx - 1] = text or ""
                    
                    # ä¿å­˜æ¯é¡µ OCR ç»“æœ
                    page_txt_path = ocr_dir / f"page_{page_idx}.txt"
                    page_txt_path.write_text(text, encoding="utf-8")

            if failed_pages:
                logger.warning(
                    "Paper %s: %d pages failed OCR: %s",
                    paper_id,
                    len(failed_pages),
                    [f"page_{idx}" for idx, _ in failed_pages],
                )
                paper["status"] = "failed"
                result["status"] = "failed"
                if not result.get("error"):
                    result["error"] = f"Failed pages: {[idx for idx, _ in failed_pages]}"

            # å†™ full.txt ä¸ usage æ—¥å¿—ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿå°½é‡å†™å‡ºå·²æœ‰å†…å®¹ï¼‰
            full_txt_path = ocr_dir / "full.txt"
            full_txt_path.write_text("\n\n".join(page_texts), encoding="utf-8")

            usage_log_path = logs_dir / "usage.json"
            usage_log_path.write_text(
                json.dumps(
                    {
                        "arxiv_id": paper_id,
                        "index": idx,
                        "status": result["status"],
                        "usage": result["usage"],
                        "page_count": result["page_count"],
                        "error": result["error"],
                        "timestamp": datetime.utcnow().isoformat() + "Z",
                    },
                    ensure_ascii=False,
                    indent=2,
                ),
                encoding="utf-8",
            )

            # åœ¨ manifest ä¸­è®°å½• OCR ç›®å½•ä¸ full æ–‡æœ¬è·¯å¾„
            paper["ocr_dir"] = str(ocr_dir)
            paper["ocr_full_path"] = str(full_txt_path)

            logger.info(
                "Finished OCR for paper #%d (%s): pages=%d, status=%s, ocr_full=%s",
                idx,
                paper_id,
                result["page_count"],
                result["status"],
                full_txt_path,
            )

            return result

        # æ§åˆ¶å¹¶å‘åº¦ï¼Œå¯¹ 12 ç¯‡è®ºæ–‡åš OCR
        semaphore = asyncio.Semaphore(self.max_concurrent_pdfs)

        async def sem_task(idx: int, paper: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                return await process_single_paper(idx, paper)

        pdf_processing_results: List[Dict[str, Any]] = await asyncio.gather(
            *[
                sem_task(idx, paper)
                for idx, paper in enumerate(papers_manifest["papers"], start=1)
            ]
        )

        # å†™ pdf_processing.json artifact
        pdf_processing_artifact_path = artifact_dir / "pdf_processing.json"
        pdf_processing_artifact_path.write_text(
            json.dumps(pdf_processing_results, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        logger.info("âœ“ pdf_processing.json saved at %s", pdf_processing_artifact_path)

        # é‡æ–°å†™å›æ›´æ–°åçš„ papers_manifestï¼ˆåŒ…å« OCR å­—æ®µä¸ status æ›´æ–°ï¼‰
        manifest_path.write_text(
            json.dumps(papers_manifest, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        logger.info("âœ“ papers_manifest.json updated with OCR info at %s", manifest_path)

        # -------------------------
        # Step 4: Markdown ç”Ÿæˆ
        # -------------------------
        logger.info("Step 4: Emit Markdown files from OCR text")

        markdown_dir = generated_dir / "markdown"
        markdown_dir.mkdir(parents=True, exist_ok=True)

        markdown_items: List[Dict[str, Any]] = []

        for idx, paper in enumerate(papers_manifest["papers"], start=1):
            if paper.get("status") != "ok":
                continue

            ocr_full_path = paper.get("ocr_full_path")
            if not ocr_full_path or not Path(ocr_full_path).exists():
                logger.warning(
                    "Skip markdown for paper %s: missing ocr_full.txt (%s)",
                    paper.get("arxiv_id"),
                    ocr_full_path,
                )
                paper["status"] = "failed"
                continue

            try:
                ocr_text = Path(ocr_full_path).read_text(encoding="utf-8")
            except Exception as e:
                logger.exception(
                    "Failed to read OCR full text for paper %s: %s",
                    paper.get("arxiv_id"),
                    e,
                )
                paper["status"] = "failed"
                continue

            # å¯¹ OCR æ–‡æœ¬åšä¸€æ¬¡ç®€å•æ¸…ç†ï¼šå‹ç¼©å¤šä½™ç©ºè¡Œ & æ˜æ˜¾å™ªéŸ³è¡Œï¼ˆä¾‹å¦‚åªæœ‰ "\" çš„è¡Œï¼‰ï¼Œé¿å…ç”Ÿæˆçš„ Markdown ä¸­å‡ºç°æˆç‰‡ç©ºç™½
            cleaned_lines: list[str] = []
            prev_blank = False
            for line in ocr_text.splitlines():
                stripped = line.strip()

                # å°†ã€Œç©ºè¡Œã€å’ŒæŸäº›å…¸å‹ OCR å™ªéŸ³è¡Œç»Ÿä¸€è§†ä¸ºç©ºè¡Œï¼Œä¾‹å¦‚åªåŒ…å« "\" çš„è¡Œ
                is_blank_or_noise = stripped == "" or stripped == "\\"

                if is_blank_or_noise:
                    # å¦‚æœä¸Šä¸€è¡Œå·²ç»æ˜¯ç©ºè¡Œ/å™ªéŸ³ï¼Œåˆ™è·³è¿‡å½“å‰è¡Œï¼ˆä¿è¯è¿ç»­ç©ºè¡Œæœ€å¤š 1 è¡Œï¼‰
                    if prev_blank:
                        continue
                    prev_blank = True
                    cleaned_lines.append("")  # ç»Ÿä¸€ç”¨çœŸæ­£çš„ç©ºè¡Œå ä½
                else:
                    prev_blank = False
                    cleaned_lines.append(line.rstrip())

            cleaned_ocr_text = "\n".join(cleaned_lines).strip()

            title = paper.get("title") or "Untitled"
            arxiv_id = paper.get("arxiv_id") or f"paper_{idx:02d}"
            published = paper.get("published") or ""
            authors = paper.get("authors") or ""
            keyword = paper.get("keyword") or ""
            bibtex_path = paper.get("bibtex_path") or ""

            md_filename = f"paper_{idx:02d}_{arxiv_id}.md"
            md_path = markdown_dir / md_filename

            md_content_lines = [
                f"# {title}",
                "",
                f"- arXiv ID: {arxiv_id}",
                f"- Published: {published}",
                f"- Authors: {authors}",
                f"- Source Keyword: {keyword}",
                f"- DBLP BibTeX: {bibtex_path}",
                "",
                "## Extracted Text",
                "",
                cleaned_ocr_text,
            ]

            md_path.write_text("\n".join(md_content_lines), encoding="utf-8")

            rel_md_path = md_path.relative_to(session_folder)
            markdown_items.append(
                {
                    "index": idx,
                    "arxiv_id": arxiv_id,
                    "title": title,
                    "keyword": keyword,
                    "markdown_path": str(rel_md_path),
                    "status": paper.get("status", "ok"),
                }
            )

            logger.info(
                "Markdown generated for paper #%d (%s): %s",
                idx,
                arxiv_id,
                md_path,
            )

        # å†™ markdown_emit.json artifact
        markdown_emit_artifact_path = artifact_dir / "markdown_emit.json"
        markdown_emit_artifact_path.write_text(
            json.dumps(markdown_items, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        logger.info("âœ“ markdown_emit.json saved at %s", markdown_emit_artifact_path)

        # ç”Ÿæˆ summary/index.mdï¼ˆå¯¹é½ planï¼‰
        index_md_path = generated_dir / "index.md"
        index_lines: List[str] = []
        index_lines.append(f"# Query â†’ Markdown Summary")
        index_lines.append("")
        index_lines.append(f"**Original Query**: {original_query}")
        index_lines.append("")
        index_lines.append("## Rewrite Keywords")
        index_lines.append("")
        for kw in keywords:
            index_lines.append(f"- {kw}")

        index_lines.append("")
        index_lines.append("## Papers")
        index_lines.append("")
        index_lines.append("| # | Title | arXiv ID | Keyword | Markdown | Status |")
        index_lines.append("|---|-------|----------|---------|----------|--------|")

        for idx, paper in enumerate(papers_manifest["papers"], start=1):
            title = (paper.get("title") or "").replace("|", "\\|")
            arxiv_id = paper.get("arxiv_id") or ""
            keyword = (paper.get("keyword") or "").replace("|", "\\|")
            status_str = paper.get("status", "ok")

            md_item = next(
                (m for m in markdown_items if m["index"] == idx),
                None,
            )
            md_path_str = md_item["markdown_path"] if md_item else ""

            index_lines.append(
                f"| {idx} | {title} | {arxiv_id} | {keyword} | {md_path_str} | {status_str} |"
            )

        if status != "ok":
            index_lines.append("")
            index_lines.append(
                f"> æ³¨æ„ï¼šå…±æ‰¾åˆ° {len(deduped_papers)} ç¯‡å»é‡è®ºæ–‡ï¼Œå°‘äºç›®æ ‡ {target_paper_count} ç¯‡ï¼Œstatus = {status}ã€‚"
            )

        failed_papers = [
            (i, p)
            for i, p in enumerate(papers_manifest["papers"], start=1)
            if p.get("status") != "ok"
        ]
        if failed_papers:
            index_lines.append("")
            index_lines.append("## Failed or Missing Entries")
            index_lines.append("")
            for idx, p in failed_papers:
                index_lines.append(
                    f"- #{idx} {p.get('title') or p.get('arxiv_id')} - status={p.get('status')}"
                )

        index_md_path.write_text("\n".join(index_lines), encoding="utf-8")
        logger.info("âœ“ index.md saved at %s", index_md_path)

        step_inputs = SessionStepInputs(
            session_folder=session_folder,
            generated_dir=generated_dir,
            artifact_dir=artifact_dir,
            markdown_items=markdown_items,
            keywords=keywords,
        )

        # -------------------------
        # Step 5: Problem Statement & Methodology æå– + Experiment æå–ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
        # -------------------------
        methodology_extraction_artifact_path: Optional[str] = None
        methodology_items: List[Dict[str, Any]] = []
        experiment_extraction_artifact_path: Optional[str] = None
        experiment_items: List[Dict[str, Any]] = []
        
        if self.methodology_extraction_agent is not None or self.experiment_extraction_agent is not None:
            logger.info("Step 5: Extract problem statements & methodologies + experiments from Markdown files (parallel)")

            # å‡†å¤‡å¹¶è¡Œä»»åŠ¡åˆ—è¡¨
            gather_tasks = []
            has_methodology = self.methodology_extraction_agent is not None
            has_experiment = self.experiment_extraction_agent is not None

            if has_methodology:
                gather_tasks.append(run_methodology_extraction_step(
                step_inputs=step_inputs,
                methodology_agent=self.methodology_extraction_agent,
                max_concurrent_tasks=self.max_concurrent_pdfs,
                ))
            else:
                gather_tasks.append(None)

            if has_experiment:
                gather_tasks.append(run_experiment_extraction_step(
                    step_inputs=step_inputs,
                    experiment_agent=self.experiment_extraction_agent,
                    max_concurrent_tasks=self.max_concurrent_pdfs,
                ))
            else:
                gather_tasks.append(None)

            # å¹¶è¡Œæ‰§è¡Œï¼ˆè¿‡æ»¤æ‰ Noneï¼‰
            results = await asyncio.gather(
                *[task for task in gather_tasks if task is not None],
                return_exceptions=True,
            )

            # å¤„ç†ç»“æœ
            result_idx = 0
            if has_methodology:
                try:
                    result = results[result_idx]
                    result_idx += 1
                    if isinstance(result, Exception):
                        logger.error("Step 5 methodology extraction failed: %s", result)
                    else:
                        methodology_extraction_artifact_path, methodology_items = result
                        logger.info("Methodology artifact: %s", methodology_extraction_artifact_path)
                except Exception as e:
                    logger.error("Error processing methodology result: %s", e)

            if has_experiment:
                try:
                    result = results[result_idx]
                    if isinstance(result, Exception):
                        logger.error("Step 5 experiment extraction failed: %s", result)
                    else:
                        experiment_extraction_artifact_path, experiment_items = result
                        logger.info("Experiment artifact: %s", experiment_extraction_artifact_path)
                except Exception as e:
                    logger.error("Error processing experiment result: %s", e)
        else:
            logger.info("Step 5: Skipped (methodology_extraction_agent and experiment_extraction_agent not provided)")

        # -------------------------
        # Step 6: Innovation synthesis agentï¼ˆ3-paper requirementï¼‰
        # -------------------------
        innovation_artifact_paths: List[str] = []
        if self.innovation_agent is not None:
            if not methodology_items:
                logger.warning("Innovation agent skipped: methodology step produced 0 eligible entries.")
            else:
                innovation_artifact_paths = await run_innovation_synthesis_step(
                    step_inputs=step_inputs,
                    methodology_items=methodology_items,
                    innovation_agent=self.innovation_agent,
                    override_keywords=innovation_keywords_override,
                    run_count=innovation_run_count,
                )
        else:
            logger.info("Step 6: Skipped (innovation_agent not provided)")

        return {
            "session_id": session_id,
            "session_folder": str(session_folder),
            "rewrite_artifact": str(rewrite_artifact_path),
            "papers_manifest": str(manifest_path),
            "pdf_processing_artifact": str(pdf_processing_artifact_path),
            "markdown_emit_artifact": str(markdown_emit_artifact_path),
            "index_md": str(index_md_path),
            "methodology_extraction_artifact": methodology_extraction_artifact_path,
            "experiment_extraction_artifact": experiment_extraction_artifact_path,
            "methodology_items": methodology_items,
            "experiment_items": experiment_items,
            "innovation_artifacts": innovation_artifact_paths,
            "innovation_artifact": innovation_artifact_paths[0] if innovation_artifact_paths else None,
            "status": status,
        }


_VISION_TEST_IMAGE_PATH = Path(__file__).resolve().parents[3] / "lab" /"img.png" #"arxiv_2506.06962v3_page_18.png"


async def _test_anthropic_connectivity(anthropic_service: AnthropicService) -> bool:
    """
    å‘é€ä¸€ä¸ªæå°çš„ messages.create è¯·æ±‚ï¼Œå¿«é€ŸéªŒè¯ Anthropic API æ˜¯å¦å¯ç”¨ã€‚
    é‡åˆ°æ— æ•ˆ token/ç½‘ç»œé”™è¯¯æ—¶æå‰ç»ˆæ­¢ main æµ‹è¯•ï¼Œé¿å…æ•´æ¡æµæ°´çº¿å¤±è´¥åˆ° OCR é˜¶æ®µæ‰å‘ç°ã€‚
    """
    logger.info("Running Anthropic connectivity test...")
    try:
        await anthropic_service.messages_create(
            messages=[{"role": "user", "content": "Ping. Reply with PONG."}],
            temperature=0,
            max_tokens=5,
            model=settings.anthropic_model,
            system="You are a simple health-check bot. Respond with 'PONG'.",
        )
        logger.info("Anthropic connectivity test succeeded.")
        return True
    except Exception as exc:
        logger.error("Anthropic connectivity test failed: %s", exc)
        return False


async def _test_vision_agent(vision_agent: VisionAgent) -> bool:
    """
    ä½¿ç”¨ lab/1.png è°ƒç”¨ VisionAgent.analyze_imageï¼ŒéªŒè¯ Anthropic è¯»å›¾æ¥å£è¿é€šæ€§ã€‚
    å›¾ç‰‡å†…å®¹ä¸ºé¡¹ç›®å†…ç½®ç¤ºä¾‹ï¼Œä»…ç”¨äºç¡®è®¤ API å¯æ¥å—å›¾ç‰‡è¾“å…¥å¹¶è¿”å›æ–‡æœ¬ã€‚
    """
    logger.info("Running Anthropic vision (image/OCR) test...")
    if not _VISION_TEST_IMAGE_PATH.exists():
        logger.error("Vision test image not found: %s", _VISION_TEST_IMAGE_PATH)
        return False
    try:
        test_image_bytes = _VISION_TEST_IMAGE_PATH.read_bytes()
        result = await vision_agent.analyze_image(
            images=[test_image_bytes],
            temperature=0,
            max_tokens=1000,
        )
        logger.info("Vision test succeeded. ResponseğŸ˜€: %s", result)
        return True
    except Exception as exc:
        logger.error("Anthropic vision test failed: %s", exc)
        return False


async def main() -> None:
    """
    ç®€å•æœ¬åœ°æµ‹è¯•å…¥å£ï¼šç›´æ¥åœ¨å½“å‰è„šæœ¬å†…è·‘ä¸€æ¬¡ Query â†’ Markdown å·¥ä½œæµã€‚
    ä¸ä½¿ç”¨ CLI/argparseï¼Œæ–¹ä¾¿åœ¨ IDE ä¸­è¿è¡Œå’Œæ–­ç‚¹è°ƒè¯•ã€‚
    
    å‚æ•°è®¾ç½®è¯´æ˜ï¼š
    ============
    1. session_id å’Œè¾“å‡ºä½ç½®çš„å…³ç³»ï¼š
       - å¦‚æœ test_session_id = Noneï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆæ ¼å¼ä¸º session_{timestamp}_{uuid} çš„ session_id
       - ä¾‹å¦‚ï¼šsession_20251127_112630_748edba5ï¼ˆ2025å¹´11æœˆ27æ—¥ 11:26:30ï¼ŒUUIDå‰8ä½ï¼‰
       - session_id å†³å®šäº†æ‰€æœ‰è¾“å‡ºæ–‡ä»¶çš„å­˜å‚¨ä½ç½®ï¼š{output_dir}/{username}/{session_id}/
       - æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆPDFã€BibTeXã€Markdownç­‰ï¼‰éƒ½ä¼šä¿å­˜åœ¨è¿™ä¸ª session æ–‡ä»¶å¤¹ä¸‹
    
    2. skip_dblp_check å‚æ•°çš„å½±å“ï¼š
       - skip_dblp_check=Falseï¼ˆé»˜è®¤ï¼‰ï¼šåªä¸‹è½½åœ¨ DBLP ä¸­æœ‰åŒ¹é…çš„è®ºæ–‡ï¼Œä½¿ç”¨ DBLP çš„ BibTeX
       - skip_dblp_check=Trueï¼šè·³è¿‡ DBLP æ£€æŸ¥ï¼Œä¸‹è½½æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼Œä½¿ç”¨ arXiv ç”Ÿæˆçš„ BibTeX
       - è®¾ç½®ä¸º True æ—¶ï¼Œå¯èƒ½ä¼šä¸‹è½½æ›´å¤šè®ºæ–‡ï¼ˆå› ä¸ºä¸é™åˆ¶ DBLP åŒ¹é…ï¼‰ï¼Œä½† BibTeX è´¨é‡å¯èƒ½è¾ƒä½
    
    3. è®ºæ–‡æ•°é‡æ§åˆ¶å‚æ•°ï¼š
       - target_paper_count=4ï¼šæœ€ç»ˆä¿ç•™çš„è®ºæ–‡æ•°é‡ï¼ˆå»é‡åï¼ŒæŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼Œå–å‰ N ç¯‡ï¼‰
       - per_keyword_max_results=3ï¼šæ¯ä¸ªå…³é”®è¯æœç´¢æ—¶è¿”å›çš„æœ€å¤§ç»“æœæ•°
       - per_keyword_recent_limit=3ï¼šæ¯ä¸ªå…³é”®è¯åªè€ƒè™‘æœ€è¿‘ N ç¯‡è®ºæ–‡
       - å®é™…æµç¨‹ï¼š4ä¸ªå…³é”®è¯ Ã— æ¯ä¸ªæœ€å¤š3ç¯‡ = æœ€å¤š12ç¯‡ â†’ å»é‡ â†’ æŒ‰æ—¶é—´æ’åº â†’ å–å‰4ç¯‡
    
    4. è¾“å‡ºæ–‡ä»¶ä½ç½®ï¼ˆä»¥ session_20251127_112630_748edba5 ä¸ºä¾‹ï¼‰ï¼š
       - raw_pdfs/ï¼šä¸‹è½½çš„åŸå§‹ PDF æ–‡ä»¶
       - generated/papers_manifest.jsonï¼šè®ºæ–‡æ¸…å•ï¼ˆåŒ…å«å…ƒæ•°æ®ã€è·¯å¾„ç­‰ï¼‰
       - generated/markdown/ï¼šç”Ÿæˆçš„ Markdown æ–‡ä»¶
       - generated/index.mdï¼šæ±‡æ€»ç´¢å¼•æ–‡ä»¶
       - artifact/ï¼šä¸­é—´äº§ç‰©ï¼ˆrewrite.jsonã€pdf_processing.json ç­‰ï¼‰
    """
    start_time = time.perf_counter()
    try:
        #test_query="""sciäºŒåŒº[é¢†åŸŸéœ€æ±‚]Autonomous driving safety"""

        # test_query="""sciäºŒåŒº[é¢†åŸŸéœ€æ±‚]AI safety protection algorithms"""

        # test_query="""Cloud computing + autonomous vehicles"""
#         test_query="""Adaptive Multi-Agent Embodied AI with Cross-Domain Visual Planning
#
#
# Current embodied AI systems cannot integrate visual understanding, cross-domain planning, and multi-agent coordination, limiting their ability to perform complex real-world tasks that require both physical actions and digital information retrieval.
#
#
# We solve the problem of fragmented embodied AI systems that cannot handle complex real-world tasks requiring visual understanding, web information, and coordinated actions. Current methods fail because they use fixed visual processing that misses temporal dynamics, make poor decisions about when to switch between physical and digital actions, and lack fault tolerance in multi-agent scenarios. Our method works in three stages: (1) Adaptive visual processing that samples video frames at 1-10fps based on motion detection and uses curriculum learning to adjust training difficulty, (2) Confidence-based cross-domain planning that calculates uncertainty scores to decide when to switch between physical actions and web queries, and (3) Fault-tolerant multi-agent coordination with heartbeat monitoring that detects agent failures in 5 seconds and reassigns tasks automatically. To implement this, we extend VideoLLaMA3 with adaptive sampling, add entropy-based confidence estimation to cross-domain planners, and build heartbeat monitoring into ROS 2 agent frameworks. We test on cooking tasks (using recipes from web), navigation with real-time map data, and warehouse coordination scenarios using AI2-THOR simulation and real robot platforms. The system needs PyTorch, ROS 2, web API access, 12GB GPU memory, and takes 2-3 days to train on video datasets with multi-agent interaction logs. Success is measured by task completion rate, domain switching accuracy, and system uptime during agent failures."""


        test_query="""logistics: A Multi-Agent Predictive QptimizationFramework"""
        test_username = "2025_12_1_lab"
        test_session_id: Optional[str] = None  # None æ—¶ä¼šè‡ªåŠ¨ç”Ÿæˆï¼Œæ ¼å¼ï¼šsession_{timestamp}_{uuid}

        _load_local_env_file()

        # æ„é€ ä¾èµ–
        openai_service = OpenAIService()
        query_agent = QueryRewriteAgent(openai_service=openai_service)
        methodology_agent = MethodologyExtractionAgent(openai_service=openai_service)
        experiment_agent = ExperimentExtractionAgent(openai_service=openai_service)
        innovation_agent = InnovationSynthesisAgent(openai_service=openai_service)

        anthropic_service = AnthropicService()
        vision_agent = VisionAgent(anthropic_service=anthropic_service)


        if not await _test_anthropic_connectivity(anthropic_service):
            logger.error("Abort workflow run due to Anthropic connectivity failure.")
            return
        if not await _test_vision_agent(vision_agent):
            logger.error("Abort workflow run due to Anthropic vision failure.")
            return

        workflow = QueryToMarkdownWorkflow(
            query_rewrite_agent=query_agent,
            vision_agent=vision_agent,
            methodology_extraction_agent=methodology_agent,
            experiment_extraction_agent=experiment_agent,
            innovation_agent=innovation_agent,
            max_concurrent_pdfs=2,
            max_concurrent_pages=5,  # æ¯ç¯‡è®ºæ–‡åŒæ—¶å¤„ç†çš„é¡µé¢æ•°
        )

        result = await workflow.execute(
            original_query=test_query,
            session_id=test_session_id,  # None æ—¶è‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚ï¼šsession_20251127_112630_748edba5
            username=test_username,
            target_paper_count=3,  # æœ€åéœ€è¦çš„æ•°é‡ï¼ˆå»é‡åæŒ‰æ—¶é—´æ’åºå–å‰ N ç¯‡ï¼‰
            per_keyword_max_results=4,  # æ¯ä¸ªå…³é”®è¯æœ€å¤§çš„æœç´¢ç»“æœ
            per_keyword_recent_limit=3,  # æ¯ä¸ªå…³é”®è¯åªè€ƒè™‘æœ€è¿‘ N ç¯‡
            skip_dblp_check=True,  # è®¾ç½®ä¸º True å¯è·³è¿‡ DBLP æ£€æŸ¥ï¼ˆä¼šä¸‹è½½æ›´å¤šè®ºæ–‡ï¼Œä½†ä½¿ç”¨ arXiv BibTeXï¼‰
        )

        logger.info("Queryâ†’Markdown workflow finished.")
        logger.info("Session folder: %s", result["session_folder"])
        logger.info("rewrite_artifact: %s", result["rewrite_artifact"])
        logger.info("papers_manifest: %s", result["papers_manifest"])
        logger.info("pdf_processing_artifact: %s", result["pdf_processing_artifact"])
        logger.info("markdown_emit_artifact: %s", result["markdown_emit_artifact"])
        logger.info("index_md: %s", result["index_md"])
        if result.get("methodology_extraction_artifact"):
            logger.info("methodology_extraction_artifact: %s", result["methodology_extraction_artifact"])
        if result.get("experiment_extraction_artifact"):
            logger.info("experiment_extraction_artifact: %s", result["experiment_extraction_artifact"])
        innovation_artifacts = result.get("innovation_artifacts") or []
        if innovation_artifacts:
            logger.info("innovation_artifacts: %s", innovation_artifacts)
    finally:
        elapsed = time.perf_counter() - start_time
        logger.info(
            "Queryâ†’Markdown workflow total runtime: %.2f seconds (â‰ˆ%.2f minutes)",
            elapsed,
            elapsed / 60,
        )


if __name__ == "__main__":
    asyncio.run(main())


