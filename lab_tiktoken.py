import tiktoken

string="""{
  "method_context": {
    "research_question": "How can we automatically detect and measure politicization across diverse social media platforms by combining topic shift detection, multi-task learning for content classification, and cross-platform boundary analysis?",
    "problem_gap": "Current methods for detecting politicization face three critical limitations: (1) they rely on expensive manual labeling or keyword matching that cannot scale across diverse non-political topics, (2) they treat political content detection as an isolated task without leveraging related classification problems like stress/depression detection that share similar linguistic patterns, and (3) they ignore how politicization manifests differently within platform-specific communities versus across platform boundaries. Existing approaches either use simple keyword matching (missing implicit politicization), train separate models for each detection task (wasting shared knowledge), or analyze single platforms in isolation (missing cross-community dynamics). No existing method combines unsupervised topic shift detection with multi-task learning and cross-platform boundary analysis to capture politicization at multiple scales.",
    "target_scenario": "Social media datasets from multiple platforms (YouTube, Twitter, TikTok, Reddit, Mastodon) during politically charged periods like elections, where users discuss both political and non-political topics. Specifically: news posts with 100-500 comments, user-generated content from 1000-100000 users across 10-1000 communities/instances, text lengths 50-5000 characters. The method applies to scenarios where researchers need to track politicization trends over time (weekly/monthly), identify which non-political topics become politicized, and understand how politicization spreads within versus across community boundaries.",
    "keywords_alignment": "This method advances data science by combining computational methods for social network analysis with machine learning for automated content classification. It enables causal analysis of how advertising and platform design affect politicization patterns, provides analytics for measuring social media effects on political discourse, and supports optimization of content moderation strategies through automated detection of topic shifts across decentralized social networks."
  },
  "module_blueprints": {
    "modules": [
      {
        "id": "A",
        "paper_reference": "[Paper A]",
        "original_role": "Module A detects politicization by identifying topic shifts in social media comments. It works in three stages: (1) uses Positive-Unlabeled (PU) Learning with seed keywords (e.g., 'Lula', 'Bolsonaro', '#eleicoes2022') to classify posts as political/non-political without full manual labeling, (2) applies BERTopic to extract interpretable topics from news posts, (3) measures politicization by calculating the percentage of comments classified as political under non-political news posts. Used for analyzing YouTube, Twitter, and TikTok data during Brazilian 2022 elections.",
        "key_mechanism": "Step 1: Identify positive examples using high-precision keywords (posts mentioning 'Lula', 'Bolsonaro', '#eleicoes2022'). Step 2: Two-step PU Learning - (a) randomly select 10% of positive examples as 'spies', train Naive Bayes on remaining positives and all unlabeled data with TF-IDF features, classify spies to find threshold t where 15% of spies fall below it, label unlabeled examples below t as reliable negatives; (b) train XGBoost classifier with word2vec features on positive examples and reliable negatives from step (a). Step 3: Apply trained classifier to all posts and comments. Step 4: Use BERTopic with BERT embeddings and c-TF-IDF to extract topics from posts (minimum 100 posts per topic). Step 5: For each non-political post topic, calculate percentage of comments classified as political - this percentage measures politicization of that topic.",
        "weaknesses": [
          {
            "id": "W-A1",
            "description": "The two-step PU Learning requires manual selection of seed keywords ('Lula', 'Bolsonaro', '#eleicoes2022') which are context-specific and must be redefined for each new political context, election, or country. This makes the method non-transferable - analyzing US elections would require identifying new candidates, UK elections different parties, etc. The paper shows that adding more keywords (from 3 to 11) increases recall but decreases precision, creating a manual tuning problem."
          },
          {
            "id": "W-A2",
            "description": "The method treats political classification as a single isolated task, training only on political vs non-political labels. It ignores that political content often contains emotional language (stress, anger, depression indicators) that could provide additional training signals. The XGBoost classifier with word2vec features may miss these emotional patterns because word2vec captures semantic similarity but not emotional intensity or psychological markers that frequently appear in politicized discussions."
          },
          {
            "id": "W-A3",
            "description": "Topic shift detection only measures within-platform politicization (e.g., YouTube comments on YouTube videos). It cannot detect cross-platform politicization patterns - for example, whether users who politicize sports content on Twitter also politicize entertainment content on TikTok, or whether certain user groups act as bridges spreading politicization across platforms. The method analyzes each platform's data separately without linking user behaviors across platforms."
          }
        ],
        "improvement": {
          "name": "Module A*",
          "design_changes": [
            "Replace manual keyword selection with automatic seed discovery: Use pre-trained political entity recognition models (e.g., spaCy NER fine-tuned on political texts) to automatically extract political entities (candidates, parties, policies) from a small sample (500-1000 posts). Rank entities by frequency and political relevance score (computed as TF-IDF weight multiplied by entity type weight: candidates=1.0, parties=0.8, policies=0.6). Select top-K entities (K=5-10) as seed keywords. This eliminates manual keyword engineering and makes the method transferable across contexts.",
            "Integrate emotional and psychological features into the classifier: Add a parallel feature extraction branch that computes LIWC (Linguistic Inquiry and Word Count) scores for each post, capturing 93 psychological dimensions including anger, anxiety, certainty, and power. Concatenate word2vec embeddings (300-dim) with LIWC features (93-dim) to create 393-dim feature vectors. Train XGBoost on these combined features. This captures both semantic content and emotional tone that characterizes politicized discussions.",
            "Add cross-platform user tracking: Assign anonymous user IDs by hashing usernames consistently across platforms. Build a user-platform bipartite graph where nodes are users and platforms, edges connect users to platforms they use. For each user active on multiple platforms, compute politicization consistency score: percentage of platforms where user posts political comments on non-political content. Identify cross-platform politicizers (consistency score > 0.7) who spread politicization across platform boundaries."
          ],
          "workflow_change": "A* works as follows: Step 1: Collect 500-1000 sample posts, apply spaCy NER model to extract political entities, compute TF-IDF * entity_type_weight for each entity, select top-5 entities as seeds (e.g., if analyzing US elections, might auto-discover 'Biden', 'Trump', 'Democrats', 'Republicans', '#election2024'). Step 2: Use these seeds to identify positive examples (posts mentioning any seed). Step 3: Run two-step PU Learning as before but with enhanced features - extract both word2vec embeddings (300-dim) and LIWC scores (93-dim) for each post, concatenate to 393-dim vectors, train XGBoost on these vectors. Step 4: Apply classifier to all posts/comments. Step 5: For users appearing on multiple platforms, hash usernames to create consistent IDs, build user-platform graph, compute politicization consistency scores. Step 6: Run BERTopic on non-political posts, measure topic shift percentages as before, but now also report cross-platform politicization metrics (percentage of users who politicize content on 2+ platforms).",
          "math_spec": "Automatic seed selection: For entity e, score(e) = TF-IDF(e) Ã— w_type(e), where w_type(candidate)=1.0, w_type(party)=0.8, w_type(policy)=0.6. Select seeds S = {eâ‚, eâ‚‚, ..., e_K} where K=5-10 and score(e_i) â‰¥ score(e_j) for i<j. Enhanced feature vector: f = [w2v(text); LIWC(text)] âˆˆ â„Â³â¹Â³ where w2v âˆˆ â„Â³â°â° and LIWC âˆˆ â„â¹Â³. Cross-platform consistency: For user u active on platforms P_u, consistency(u) = |{p âˆˆ P_u : politicization_rate(u,p) > 0.5}| / |P_u|, where politicization_rate(u,p) = (political_comments(u,p) on non-political posts) / (total_comments(u,p) on non-political posts)."
        }
      },
      {
        "id": "B",
        "paper_reference": "[Paper B]",
        "original_role": "Module B detects stress and depression in social media posts using multi-task learning. It processes Reddit posts (max 512 tokens) through shared and task-specific BERT encoders. Two architectures are used: (1) Double Encoders - one shared BERT layer followed by two separate task-specific BERT layers (one for depression, one for stress), each with a 2-unit dense classifier; (2) Attention Fusion Network - shared BERT and task-specific BERT outputs are combined using learned attention weights (Î±_shared and Î±_task) before final classification. Trained on Dreaddit dataset (3553 stress posts) and depression dataset (2822 posts) with joint loss function.",
        "key_mechanism": "Step 1: Tokenize input post using BERT tokenizer, pad to 512 tokens, get input_ids and attention_mask. Step 2: Pass through shared BERT model, output z âˆˆ â„^(512Ã—768). Step 3a (Double Encoders): Pass z through two separate task-specific BERT models - one for depression, one for stress. Extract [CLS] token from each (768-dim vectors). Apply separate 2-unit dense layers with softmax for depression and stress predictions. Step 3b (Attention Fusion): Pass z through task-specific BERT, extract [CLS] token. Concatenate shared [CLS] and task-specific [CLS] (1536-dim), pass through two dense layers (768 units with ReLU, then 128 units with ReLU), then 2-unit dense layer with softmax to get attention weights Î±_shared and Î±_task. Compute final representation as Î±_shared Ã— shared_CLS + Î±_task Ã— task_CLS. Apply 2-unit dense classifier. Step 4: Optimize joint loss L = (1-Î²)L_depression + Î²L_stress where both losses are cross-entropy, Î²=0.01 for Double Encoders, Î²=0.1 for Attention Fusion.",
        "weaknesses": [
          {
            "id": "W-B1",
            "description": "The method requires two separate labeled datasets - one for depression (2822 posts) and one for stress (3553 posts). This limits applicability to scenarios where multiple labeled datasets exist. For political content detection, we typically have only one labeled dataset (political vs non-political), not separate datasets for different aspects of politicization. The architecture cannot leverage multi-task learning when only a single task dataset is available."
          },
          {
            "id": "W-B2",
            "description": "The shared BERT encoder processes all posts identically regardless of their source platform or context. Posts from Twitter (280 char limit, hashtag-heavy), Reddit (long-form discussions), and TikTok (short captions with emojis) have very different linguistic styles, but the model treats them uniformly. This causes the shared encoder to learn averaged representations that may be suboptimal for any specific platform. The paper reports results only on Reddit data, not testing cross-platform generalization."
          },
          {
            "id": "W-B3",
            "description": "The attention fusion mechanism learns only two scalar weights (Î±_shared and Î±_task) that are applied uniformly to entire 768-dim [CLS] vectors. This coarse-grained weighting cannot capture that some dimensions of the shared representation may be more useful than others for a specific task. For example, dimensions capturing emotional tone might be highly relevant for stress detection but less so for depression detection, but the model applies the same Î±_shared weight to all 768 dimensions."
          }
        ],
        "improvement": {
          "name": "Module B*",
          "design_changes": [
            "Reformulate as auxiliary task learning: Instead of requiring two separate labeled datasets, use the main task (political classification) with full labels and treat emotional/psychological features as auxiliary self-supervised tasks. Specifically, use LIWC to automatically compute emotional scores (anger, anxiety, certainty) for all posts, creating pseudo-labels. Train main task (political classification) jointly with auxiliary tasks (predicting LIWC emotional scores as regression targets). This enables multi-task learning with only one manually labeled dataset.",
            "Add platform-specific encoding layers: Before the shared BERT encoder, add platform-specific embedding layers that transform input tokens based on platform characteristics. For Twitter: add hashtag embeddings (learn 128-dim vectors for top-1000 hashtags). For Reddit: add subreddit embeddings (128-dim vectors for top-500 subreddits). For TikTok: add emoji embeddings (128-dim vectors for top-200 emojis). Concatenate these platform-specific embeddings with BERT token embeddings before feeding to shared encoder. This allows the model to learn platform-specific patterns while still sharing high-level representations.",
            "Replace scalar attention with dimension-wise attention: Instead of learning two scalars (Î±_shared, Î±_task), learn two 768-dim weight vectors (w_shared âˆˆ â„â·â¶â¸, w_task âˆˆ â„â·â¶â¸). Apply element-wise multiplication: final_representation = w_shared âŠ™ shared_CLS + w_task âŠ™ task_CLS, where âŠ™ is element-wise product. This allows the model to weight different dimensions differently - for example, giving high weight to emotion-related dimensions for stress detection but low weight for depression detection."
          ],
          "workflow_change": "B* works as follows: Step 1: Tokenize input post (max 512 tokens), identify platform (Twitter/Reddit/TikTok). Step 2: Extract platform-specific features - for Twitter extract hashtags, for Reddit extract subreddit, for TikTok extract emojis. Look up corresponding embeddings (128-dim) from learned embedding tables. Step 3: Concatenate platform embeddings with BERT token embeddings: enhanced_embedding = [BERT_embedding(token); platform_embedding] âˆˆ â„â¸â¹â¶. Step 4: Pass through shared BERT encoder (modified to accept 896-dim inputs), output z âˆˆ â„^(512Ã—768). Step 5: Pass z through task-specific BERT, extract [CLS] tokens from both (shared_CLS, task_CLS âˆˆ â„â·â¶â¸). Step 6: Concatenate [shared_CLS; task_CLS] (1536-dim), pass through dense layers (768 units ReLU, 128 units ReLU), then two parallel 768-unit dense layers with sigmoid to get w_shared and w_task âˆˆ â„â·â¶â¸. Step 7: Compute final_representation = w_shared âŠ™ shared_CLS + w_task âŠ™ task_CLS. Step 8: Apply 2-unit dense classifier for main task (political classification). Step 9: Apply 3-unit regression head for auxiliary tasks (predicting LIWC anger, anxiety, certainty scores). Step 10: Optimize L = (1-Î²)L_main + Î²(L_anger + L_anxiety + L_certainty) where L_main is cross-entropy, others are MSE, Î²=0.1.",
          "math_spec": "Platform-specific embedding: e_platform = Embed_platform(features) âˆˆ â„Â¹Â²â¸, where features are hashtags (Twitter), subreddit (Reddit), or emojis (TikTok). Enhanced input: x_enhanced = [x_BERT; e_platform] âˆˆ â„â¸â¹â¶. Dimension-wise attention: w_shared, w_task = Ïƒ(Denseâ‚â‚‚â‚ˆ(ReLU(Denseâ‚‡â‚†â‚ˆ([shared_CLS; task_CLS])))), where Ïƒ is sigmoid, Dense_n is n-unit dense layer. Final representation: h = w_shared âŠ™ shared_CLS + w_task âŠ™ task_CLS âˆˆ â„â·â¶â¸. Joint loss: L = (1-Î²)CE(y_political, Å·_political) + Î²âˆ‘_{sâˆˆ{anger,anxiety,certainty}} MSE(LIWC_s(text), Å·_s), where Î²=0.1, CE is cross-entropy, MSE is mean squared error."
        }
      },
      {
        "id": "C",
        "paper_reference": "[Paper C]",
        "original_role": "Module C analyzes user roles and information flow in decentralized social networks (Mastodon). It builds directed user networks from 1.4M users and 18M following relationships across 16,000 instances. Three main analyses: (1) Structural analysis - computes network metrics (degree distribution, clustering, community detection using Louvain/Infomap/Leiden algorithms) to characterize network topology; (2) Boundary spanning - identifies bridge users using node-centric Directed Topological Overlap (nDTO) score, where lower nDTO indicates stronger bridges connecting different communities; (3) Information consumption - applies LurkerRank algorithm to identify lurkers (users who consume more than produce), measuring information flow within and across instance boundaries.",
        "key_mechanism": "Step 1: Build directed graph G=(V,E) where nodes are (user, instance) pairs and edges are following relationships. Step 2: Compute structural metrics - degree distribution, clustering coefficient (average over nodes with degree>1), reciprocity (fraction of reciprocal edges), community detection (run Louvain, Infomap, Leiden algorithms, report number of communities and modularity). Step 3: Identify bridges using nDTO - for each node u, compute nDTO(u) = (1/|neighbors(u)|) Ã— (Î£_{vâˆˆin-neighbors} DTO(v,u) + Î£_{vâˆˆout-neighbors} DTO(u,v)), where DTO(u,v) = |out-neighbors(u) âˆ© in-neighbors(v)| / (|out-neighbors(u)| + |in-neighbors(v)| - |out-neighbors(u) âˆ© in-neighbors(v)| - 1). Nodes with nDTO=0 are strong bridges. Rank all nodes by nDTO, select bottom 5%/10%/25% as bridges. Step 4: Identify lurkers using LurkerRank - compute LR(v) = Î±[L_in(v)(1+L_out(v))] + (1-Î±)p(v), where L_in(v) = (1/|out-neighbors(v)|) Ã— Î£_{uâˆˆin-neighbors(v)} (|out-neighbors(u)|/|in-neighbors(u)|) Ã— LR(u), L_out(v) = (|in-neighbors(v)|/Î£_{wâˆˆout-neighbors(v)} |in-neighbors(w)|) Ã— Î£_{uâˆˆout-neighbors(v)} (|in-neighbors(u)|/|out-neighbors(u)|) Ã— LR(u), Î±=0.85, p(v)=1/|V|. Rank nodes by LR, select top 5%/10%/25% as lurkers. Step 5: Analyze dual roles - compute intersection of lurker and bridge sets at matching percentiles (e.g., top-5% lurkers âˆ© bottom-5% bridges).",
        "weaknesses": [
          {
            "id": "W-C1",
            "description": "The nDTO bridge detection and LurkerRank algorithms operate only on network topology (following relationships), completely ignoring content. A user might follow many political accounts and post political comments (acting as a political bridge), but nDTO treats them identically to a user who follows cooking accounts and posts recipes. The method cannot distinguish between users who bridge political information versus users who bridge non-political information, making it impossible to identify specifically political boundary spanners."
          },
          {
            "id": "W-C2",
            "description": "LurkerRank computation requires iterative calculation over the entire network graph until convergence, with complexity O(|E| Ã— num_iterations) where |E|=18M edges and num_iterations typically 20-50. For the full Mastodon network, this takes hours on a single machine. The paper only reports results for top-5 instances (merged network with ~100k nodes), not the full 1.4M node network. Scaling to multiple platforms (adding Twitter's 500M users, YouTube's 2B users) would be computationally infeasible."
          },
          {
            "id": "W-C3",
            "description": "The method analyzes each instance or merged network independently, computing separate nDTO and LurkerRank scores for each network. It cannot track how a user's role changes across different contexts - for example, a user might be a lurker in political discussions (consuming political content) but a bridge in entertainment discussions (connecting entertainment communities). The paper only compares local (instance-level) versus global (merged network) roles, not topic-specific roles within the same network."
          }
        ],
        "improvement": {
          "name": "Module C*",
          "design_changes": [
            "Add content-aware bridge detection: Instead of computing nDTO on the full following graph, first partition edges by content type using the political classifier from A*. Create two subgraphs: G_political (edges where source user posts political content) and G_non-political (edges where source posts non-political content). Compute separate nDTO scores on each subgraph: nDTO_political and nDTO_non-political. Identify political bridges as users with low nDTO_political (bottom 10%) and high nDTO_non-political (top 50%), meaning they specifically bridge political communities while not bridging non-political ones.",
            "Replace iterative LurkerRank with approximation: Instead of iterating until convergence, use a 2-hop approximation. For each user v, compute local lurker score: LLS(v) = (|in-neighbors(v)| / |out-neighbors(v)|) Ã— (avg_out-degree of in-neighbors / avg_in-degree of out-neighbors). This captures the core intuition (consuming from high-producers, producing to low-consumers) without iteration. Complexity reduces from O(|E| Ã— num_iterations) to O(|E|), enabling analysis of billion-edge graphs. Validate that LLS correlates >0.85 with full LurkerRank on sample networks.",
            "Add topic-specific role detection: For each user, classify their posts into topics using BERTopic (from A*). For each topic t, compute topic-specific metrics: nDTO_t (bridge score on subgraph of edges related to topic t) and LLS_t (lurker score on topic t subgraph). Create user role profiles: for each user, list (topic, role) pairs where role âˆˆ {bridge, lurker, producer, consumer}. Identify role-switching users: those who are bridges in one topic (e.g., nDTO_sports < 0.1) but lurkers in another (e.g., LLS_politics > 0.9)."
          ],
          "workflow_change": "C* works as follows: Step 1: Build directed graph G=(V,E) from following relationships. Step 2: For each edge (u,v), classify user u's posts using political classifier from A*, label edge as political if >50% of u's posts are political, otherwise non-political. Create G_political and G_non-political subgraphs. Step 3: Compute nDTO on each subgraph separately. For G_political, compute nDTO_political(u) for each user u. For G_non-political, compute nDTO_non-political(u). Identify political bridges as users with nDTO_political in bottom 10% AND nDTO_non-political in top 50%. Step 4: Compute local lurker score LLS(v) = (|in-neighbors(v)| / (|out-neighbors(v)| + 1)) Ã— (Î£_{uâˆˆin-neighbors(v)} |out-neighbors(u)| / |in-neighbors(v)|) / (Î£_{wâˆˆout-neighbors(v)} |in-neighbors(w)| / |out-neighbors(v)|) for each user v. Rank users by LLS, select top 10% as lurkers. Step 5: Apply BERTopic to all posts, assign each post to a topic. For each user u and topic t, compute nDTO_t(u) on subgraph of edges where source posts about topic t, and LLS_t(u) on same subgraph. Step 6: Create role profile for each user: list of (topic, role) pairs where role is determined by thresholds (nDTO_t < 0.1 â†’ bridge, LLS_t > 0.9 â†’ lurker, etc.). Step 7: Identify role-switching users: those with bridge role in one topic and lurker role in another.",
          "math_spec": "Content-aware bridge detection: Partition edges E = E_political âˆª E_non-political where (u,v) âˆˆ E_political iff (Î£_{pâˆˆposts(u)} ğŸ™[political(p)]) / |posts(u)| > 0.5. Compute nDTO_political(u) on G_political = (V, E_political) and nDTO_non-political(u) on G_non-political = (V, E_non-political). Political bridges: B_political = {u : nDTO_political(u) â‰¤ percentile(nDTO_political, 10) âˆ§ nDTO_non-political(u) â‰¥ percentile(nDTO_non-political, 50)}. Local lurker score: LLS(v) = (|N_v^in| / (|N_v^out| + 1)) Ã— ((Î£_{uâˆˆN_v^in} |N_u^out|) / |N_v^in|) / ((Î£_{wâˆˆN_v^out} |N_w^in|) / |N_v^out|), where N_v^in and N_v^out are in-neighbors and out-neighbors with Laplace smoothing. Topic-specific metrics: For topic t, let E_t = {(u,v) âˆˆ E : âˆƒp âˆˆ posts(u), topic(p) = t}. Compute nDTO_t(u) on G_t = (V, E_t) and LLS_t(v) on G_t."
        }
      }
    ]
  },
  "integration_strategy": {
    "evaluated_combinations": [
      {
        "combination_id": "C1",
        "pipeline": "A* â†’ B* â†’ C*",
        "modules_used": ["A*", "B*", "C*"],
        "connection_details": "Step 1: A* processes raw social media posts. Input: JSON array of post objects [{post_id: str, text: str, platform: str, author_id: str, timestamp: ISO8601, comments: [{comment_id: str, text: str, author_id: str}]}], file size ~50-500MB for 10k-100k posts. A* extracts political entities using spaCy NER, selects top-5 seeds, runs PU Learning with word2vec+LIWC features, outputs classification results as JSON: [{post_id: str, is_political: bool, political_prob: float[0-1], topic: str, comments: [{comment_id: str, is_political: bool, political_prob: float}]}]. Also outputs cross-platform user data: [{user_id: str (hashed), platforms: [str], politicization_consistency: float[0-1]}]. Step 2: B* takes A* outputs. Loads post texts and political labels, extracts platform-specific features (hashtags for Twitter, subreddits for Reddit, emojis for TikTok) by parsing text with regex. Computes LIWC scores using Python liwc library (pip install liwc, version 0.5.0). Creates feature vectors: [word2vec_embedding (300-dim); LIWC_scores (93-dim); platform_embedding (128-dim)] = 521-dim vectors. Trains multi-task model with PyTorch 2.0.0: main task predicts is_political (cross-entropy loss), auxiliary tasks predict LIWC anger/anxiety/certainty scores (MSE loss). Outputs enhanced classifications: [{post_id: str, is_political: bool, political_prob: float, emotional_scores: {anger: float, anxiety: float, certainty: float}}]. Saves model checkpoint as .pt file (~500MB). Step 3: C* takes A* and B* outputs. Builds directed graph using NetworkX 3.1: nodes are (user_id, platform) tuples, edges are following relationships. Loads post classifications from B*, partitions edges into E_political and E_non-political based on whether source user's posts are >50% political. Computes nDTO_political and nDTO_non-political using custom Python function (iterates over edges, computes set intersections). Computes LLS using vectorized NumPy operations (version 1.24.0). Applies BERTopic (version 0.15.0) to posts, assigns topics. Computes topic-specific nDTO_t and LLS_t for each user-topic pair. Outputs user role profiles as JSON: [{user_id: str, overall_role: str, topic_roles: [{topic: str, nDTO: float, LLS: float, role: str}], is_political_bridge: bool, is_cross_platform_politicizer: bool}], file size ~10-100MB for 10k-100k users.",
        "novelty_level": "High",
        "fit_to_problem_gap": "Problem: Existing methods cannot detect politicization across diverse topics without manual labeling, ignore emotional patterns in political content, and miss cross-platform dynamics. Step 1: A* solves the labeling problem by automatically discovering political entities (no manual keyword engineering) and using PU Learning to classify posts with only positive examples. For example, analyzing 2024 US elections, A* automatically extracts 'Biden', 'Trump', 'Democrats' from 1000 sample posts, uses these as seeds for PU Learning, achieves 0.85 F1 score on political classification without manually labeling 100k posts. Step 2: B* solves the emotional pattern problem by jointly training on political classification and emotional prediction (LIWC scores). For example, on Reddit political discussions, B* learns that high anger scores (LIWC anger > 0.7) correlate with political content, improving classification F1 from 0.85 to 0.89. The platform-specific embeddings handle Twitter's hashtag-heavy style versus Reddit's long-form text, improving cross-platform F1 from 0.82 to 0.87. Step 3: C* solves the cross-platform problem by tracking users across platforms (via hashed IDs) and computing content-aware bridge scores. For example, identifies that 15% of users who politicize sports content on Twitter also politicize entertainment on TikTok (cross-platform politicizers), and 8% of users act as political bridges (low nDTO_political) while being non-political lurkers (high LLS_non-political). Together, the pipeline detects politicization at three scales: topic-level (which topics become politicized), user-level (which users spread politicization), and platform-level (how politicization flows across platforms).",
        "feasibility_notes": "Libraries: Python 3.9+, PyTorch 2.0.0, Transformers 4.30.0 (Hugging Face), spaCy 3.5.0 with en_core_web_lg model, LIWC 0.5.0, NetworkX 3.1, BERTopic 0.15.0, NumPy 1.24.0, scikit-learn 1.3.0, XGBoost 1.7.0. Hardware: GPU with 16GB+ VRAM (NVIDIA RTX 3090 or A100) for training B*, 32GB+ CPU RAM for C* graph computations, 500GB+ storage for datasets. Data requirements: Social media posts in JSON format with fields {post_id, text, platform, author_id, timestamp, comments}, minimum 10k posts for training, 100k+ for robust analysis. API access: Twitter API v2 (Academic Research track for historical data), YouTube Data API v3, TikTok unofficial API or web scraper, Mastodon REST API. Pre-trained models: BERT base uncased (110M parameters, ~440MB), word2vec GoogleNews vectors (3M words, ~3.6GB). Training time: A* PU Learning ~2 hours on 100k posts (CPU), B* multi-task training ~8 hours on 50k
2"""

def count_tokens(text: str, encoding_name: str = "cl100k_base") -> tuple[int, int]:
    """
    ç»Ÿè®¡æ–‡æœ¬çš„ Token æ•°å’Œå­—ç¬¦æ•°
    :param text: å¾…ç»Ÿè®¡æ–‡æœ¬
    :param encoding_name: ç¼–ç æ–¹æ¡ˆï¼ˆcl100k_base é€‚é… GPT-3.5/4ï¼›p50k_base é€‚é… Codexï¼›r50k_base é€‚é… GPT-2ï¼‰
    :return: (å­—ç¬¦æ•°, Token æ•°)
    """
    # åŠ è½½ç¼–ç 
    encoding = tiktoken.get_encoding(encoding_name)
    # ç»Ÿè®¡å­—ç¬¦æ•°ï¼ˆå«ç©ºæ ¼/æ ‡ç‚¹ï¼Œä¸å«æ¢è¡Œç¬¦ï¼‰
    char_count = len(text.replace("\n", ""))
    # ç»Ÿè®¡ Token æ•°
    token_count = len(encoding.encode(text))
    return char_count, token_count
print(count_tokens(string))